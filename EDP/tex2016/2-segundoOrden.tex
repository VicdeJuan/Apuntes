% -*- root: ../EDP2016.tex -*-
% clase 2016/02/15

\chapter{Ecuaciones de segundo orden. Series de Fourier}
\label{chap:EcuacionesSegundoOrden}

\section{Método de separación de variables}\index{Ecuación!del calor}

	Al empezar el curso ya vimos un ejemplo: la ecuación del calor en una dimensión con datos de contorno Dirichlet homogéneos.\index{Método!separación de variables}

	\begin{example} \concept{Ecuación\IS con contorno Dirichlet}
		\[
		\begin{cases}
		u_t - u_{xx} = 0 & x \in (0,L), \quad t > 0 \\
		u(0,t) = u(L,t) = 0 & t > 0 \\
		u(x,0) = f(0) & x \in [0,L]
		\end{cases}
		\]

		La ecuación del medio es el dato de contorno de Dirichlet\index{Condición! de contorno Dirichlet} homogéneo, es decir, que especifica el dato en los extremos.

		Llegamos con separación de variables a que la solución del problema podía ser escrita como:

		\[ u(x,t) \eqexpl{?} \sum^{\infty}_{k=1} a_k e^{-(\frac{k\pi}{L})^2 t} \sin \left( \frac{k\pi}{L} x \right) \]
		donde
		\[ f(x) \eqexpl{?} \sum^{\infty}_{k=1} a_k \sin \left( \frac{k\pi}{L}x \right) \]
	\end{example}


	\begin{example} \concept{Ecuación\IS con contorno Neumann}

		\[
		\begin{cases}
		u_t - u_{xx} = 0 & x \in (0,L), t > 0 \\
		u_x(0,t) = u_x(L,t) = 0 & t > 0 \\
		u(x,0) = f(x) & x \in [0,L]
		\end{cases}
		\]

		Esta condición indica que no hay flujo de calor entre la varilla y cualquier punto fuera, incluidos los extremos. Esperamos que al final, cuando el tiempo tienda a infinito el calor se haya distribuido a lo largo de la varilla y la temperatura sea constante a lo largo de esta. El valor de esto probablemente sea el promedio.

		\begin{figure}[thbp]
		\centering
		\inputtikz{transmisionCalorNeumann}
		\caption{}
		\label{fig:transmisionCalorNeumann}
		\end{figure}

		Empecemos con el método de separación de variables. Buscamos $u(x,t) = X(t) \cdot T(t)$ que sea solución de la ecuación con el contorno (el dato inicial se tratará después).


		\[
		\begin{array}{l}
			0 = u_t - u_{xx} = T\ ' X - T X'' \\
			0 = u_x (0,t) = T(t) X'(0) \\
			0 = u_x (L,t) = T(t) X'(L)
		\end{array}
		\]

		De lo que obtenemos:

		\[ \frac{T'(t)}{T(t)} = \frac{X''(x)}{X(x)} \quad \forall x, \forall t \]

		Como estamos igualando dos cocientes de funciones de variables diferentes que es cierto $\forall x, \forall t$, esto solo puede significar que ambos cocientes son constantes. A esta proporción la llamaremos $\lambda$:

		\[ \frac{T'}{T} = \frac{X''}{X} = \lambda \in \mathbb{R} \]

		% Método general? SI
		Resolvemos la EDO en X:

		\[
		\left\{ \begin{array}{l}
		X'' = \lambda X \\
		X'(0) = X'(L) = 0
		\end{array} \right. \quad\quad \text{(problema de contorno)}
		\]

		Veamos los distintos casos en función de $\lambda$:

		\begin{itemize}
			\item $\lambda = 0$

				Cuando $\lambda = 0 \Rightarrow X'' = 0$. Así que tenemos que $X'$ tiene que ser constante y $X$ lineal. Pero además los datos iniciales nos indican el valor de $X'$, al ser ésta constante.

				\[ \left.
				\begin{array}{l}
					X(x) = a + bx \\
					\left.
					\begin{array}{r}
						X'(x) = b \\
						X'(0) = X'(L) = 0
					\end{array} \right\} \Rightarrow b = 0
				\end{array} \right\} \Rightarrow X \equiv a \]

				Tiene una solución no trivial que es $\lambda = 0, X=a_0$.

			\item $\lambda > 0$ con $\lambda = \mu^2$, $\mu \in \mathbb{R}$

				Lo cual nos lleva a una EDO de orden 2, que se resolvería con el polinomio característico.

				\[ \text{Las soluciones siguen } \left\{
				   \begin{array}{l}
				   	X(x) = a \cdot e^{\mu x} + b \cdot e^{-\mu x} \\
				   	X'(x) = \mu (a\cdot e^{\mu x} - b\cdot e^{-\mu x})
				   \end{array} \right.
				\]

				\[ \left. \begin{array}{l}
					0 = X'(0) \Rightarrow \mu(a - b) = 0 \\
					0 = X'(L) \Rightarrow \mu(a \cdot e^{\mu L} - b \cdot e^{-\mu L}) = 0
				\end{array} \right\}
					\Rightarrow … \Rightarrow a = b = 0
				\]


			\item $\lambda < 0$ con $\lambda = - \mu^2$

				Aquí volvemos a emplear el polinomio característico pero llegaremos a soluciones complejas, así que hay que hacer el truco de parte real y parte imaginaria para obtener

			 	\[ \text{Solución} \left\{
				   \begin{array}{l}
				   	X(x) = a \cos(\mu x ) + b \sin( \mu x) \\
				   	X'(x) = -a \mu \sin(\mu x) + b \mu \cos(\mu x)
				   \end{array} \right.
				\]

			 	\[
			 		\begin{array}{l}
			 		0 = X'(0) = b \mu \\
			 		0 = X'(L)
			 		\end{array} \Rightarrow b = 0 \Rightarrow \left\{ \begin{array}{l}
			 			X(x) = + a \cos (\mu x ) \\
			 			X'(x) = -a \mu \sin (\mu x)
			 		\end{array} \right.
			 	\]

			 	De lo que obtenemos que

			 	\[0 = X'(L) = -a \mu \sin(\mu L) \Rightarrow \mu L = k \pi , \quad k = 1,2,…\]



		\end{itemize}

		{\bf Conclusión:}
				\begin{align*}
					\lambda_0 = 0\quad & \quad X_0 = a_0 \\
					\lambda_k = - \left(\frac{k\pi}{L}\right)^2\quad & \quad X_k(x) = a_k \cos \left(\frac{k \pi}{L}x\right)
				\end{align*}

			 	Resolvemos la EDO para T (para las $\lambda$ encontradas antes)

			 	\[\lambda_0 = 0 \Rightarrow T'_0 \equiv 0 \Rightarrow T_0 \equiv \alpha_0\]
			 	\[\lambda_k = - \left(\frac{k\pi}{L}\right)^2 \Rightarrow T'_k = -\left(\frac{k\pi}{L}\right)^2 T_k \Rightarrow T_k (t) = \alpha_k e^{-\left(\frac{k\pi}{L}\right)^2 t} \]

			 	Soluciones particulares:
			 	\[u_0(x,t) = A_0, \quad u_k (x,t) = A_k e^{-\left(\frac{k \pi}{L} \right)^2 t} \cos \left( \left( \frac{k \pi}{L}\right) x \right) \]

			 	Dato inicial: $u(x,0) = f(x)$

			 	Idea: $u(x,t) \eqexpl{?} A_0 + \sum\limits_{k=1}^{\infty} A_k e^{- \left( \frac{k \pi}{L} \right)^2 t}  \cos \left( \frac{k \pi}{L} x \right)$

			 	Pero claro, no sabemos calcular $A_k$. ¿O cómo demostramos la convergencia? ¿Cómo calculamos las derivadas?


		\end{example}

		\clearpage % FIXME: si se edita por encima, podría crear una página en blanco en medio del documento
		\begin{example}{\bf 3: Cuerda vibrante}

			Veamos una cuerda de guitarra en tensión. La guitarra está atada en los extremos y la altura sobre el eje horizontal es $u$.

			\begin{figure}[thbp]
			\centering
			\inputtikz{cuerdaGuitarra}
			\caption{Cuerda vibrando. $u$ es la altura}
			\label{fig:cuerdaGuitarra}
			\end{figure}

			Ahora vamos a presentar un problema de 2º orden, que requiere de 2 datos para resolverlo:
			\[  \begin{cases}
				u_{tt} - u_{xx} = 0\\
				u(0,t) = u(L,t) = 0 \quad \text{Dirichlet}\\
				u(x,0) = f(x) \\
				u_t(x,0) = g(x)
				\end{cases}
			\]

			Lo vamos a resolver por separación de variables\index{Método!separación de variables}. Buscamos un $u(x,t) = X(t) T(t)$, solución de la ecuación homogénea:
			\[ 0 = u_{tt} - u_{xx} = T'' X - T X''\]
			\[ \frac{T''}{T} = \frac{X''}{X} = \lambda \in \mathbb{R}\]

			Pasamos a resolver la EDO para $X$:
			\[\begin{cases}
				X'' = \lambda X \\
				X(0) = X(L) = 0
			\end{cases}
			\]

			Vemos qué ha cambiado respecto al sistema anterior en que la última ecuación ya no relaciona las derivadas de $X$, sino $X$. De nuevo, buscamos las soluciones en función del valor de $\lambda$:

			\begin{itemize}
				\item $\lambda = 0$
					\[
					\left\{
					\begin{array}{l}
					X(x) = a + bx \\
					X(0) = 0 = X(L)
					\end{array}
					\right.
					\Rightarrow
					a = 0 = b
					\]

				\item $\lambda > 0$ con $\lambda = \mu^2$

					\[
					\left\{
					\begin{array}{l}
					X(x) = a \cdot e^{\mu x} + b \cdot e^{-\mu x} \\
					X(0) = 0 = X(L)
					\end{array}
					\right.
					\Rightarrow … \Rightarrow
					a = b = 0
					\]

				\item $\lambda < 0$ con $\lambda = -\mu^2$

					\[
					\left\{
					\begin{array}{l}
					X(x) = a\cos(\mu x) + b\sin(\mu x) \\
					X(0) = 0 = X(L)
					\end{array}
					\right.
					\Rightarrow X(0) = a \Rightarrow X(x) = b \sin(\mu x)
					\]

					\[ \Rightarrow X(L) = 0 = b \sin (\mu L) \Rightarrow \mu = \frac{k \pi}{L}\]

			\end{itemize}

			Con lo que llegamos a las soluciones no triviales:
			\[\lambda_k = - (\frac{k\pi}{L})^2, \quad X_k(x) = b_k \sin \left(\frac{k\pi}{L} \right) x\]


			Una vez que resolvemos la EDO para $X$, la resolvemos para $T$:

			\[T'' = \lambda T\]

			Es similar a la X, así que tenemos:

			\[T''_k = - (\frac{k\pi}{L})^2 T_k \Rightarrow T_k (t) = \alpha_k \cos\left( \frac{k \pi}{L} t \right) + \beta_k \sin \left( \frac{k \pi}{L}t \right)\]

			Con lo que llegamos a las soluciones particulares:

			\[u_k(x,t) = A_k \cos \left(\frac{k\pi}{L} t\right) \sin \left(\frac{k\pi}{L}x\right) + B_k \sin \left(\frac{k\pi}{L}t\right)  \sin \left(\frac{k\pi}{L}x\right) \]

			{\bf Idea:} Buscar

			\[u(x,t) \eqexpl{?} \sum_{k=1}^{\infty} A_k \cos \left(\frac{k\pi}{L} t\right) \sin \left(\frac{k\pi}{L} x  \right)+ B_k \sin \left(\frac{k\pi}{L}t \right) \sin \left(\frac{k\pi}{L}  x \right)\]

			Datos iniciales:
			\[ f(x) = u(x,0) \eqexpl{?} \sum^{\infty}_{k=1} A_k \sin \left(\frac{k\pi}{L} x  \right)\]

			Suponiendo que derivada e integral conmutan:

			\[ u_t (x,t) \eqexpl{?} \sum_{k} - A_k \left(\frac{k\pi}{L} \right) \sin \left(\frac{k\pi}{L}t \right) \sin \left(\frac{k\pi}{L}x \right) + B_k \left(\frac{k\pi}{L} \right) \cos \left(\frac{k\pi}{L}t \right) \sin \left(\frac{k\pi}{L}x \right)
			\]

			\[g(x) = u_t(x,0) \eqexpl{?} \sum_k B_k  \left(\frac{k\pi}{L} \right) \sin \left(\frac{k\pi}{L}x \right)\]

		\end{example}

		% clase 2016/02/16

		\begin{example}{\bf 4: Ondas con condiciones periódicas}\index{Ecuación!de ondas}\label{ec:ondas}

			Estudiemos, por ejemplo, las olas en alta mar. No tenemos un contorno fijo como antes, así que vamos a buscar soluciones que sean periódicas en los extremos. En este caso tendremos dos condiciones, llamadas \concept[Condición\IS de periodicidad]{condiciones de periodicidad}. Hemos puesto dos porque la ecuación es de segundo orden:

			\[u(-L,t) = u(L,t), \forall t\]
			\[u_x(-L,t) = u_x(L,t), \forall t\]

			El problema nos queda así:
			\[  \begin{cases}
				u_{tt} - u_{xx} = 0 \quad x  \in (-L,L)\footnotemark, t>0\\
				u(-L,t) = u(L,t), \forall t\\
				u_x(-L,t) = u_x(L,t), \forall t
				u(x,0) = f(x) \\
				u_t(x,0) = g(x)
				\end{cases}
			\]
			\footnotetext{Para que las cuentas cuadren mejor}

			Aplicamos el método de separación de variables:
			\[ \frac{T''}{T} = \frac{X''}{X} = \lambda \in \mathbb{R}\]

			Comenzamos resolviendo la EDO para $X$:
			\[\left\{\begin{array}{l}
				X'' = \lambda X \quad x \in (-L,L) \\
				X(-L) = X(L) \\
				X'(-L) = X'(L)
			\end{array}
			\right. \]

			Estudiamos según los valores que pueda tomar $\lambda$:
			\begin{itemize}
				\item $\lambda = 0$
					\[\left\{\begin{array}{l}
						X = a+bx \\
						X'(x) = b \\
						X(-L) = X(L) \quad \Rightarrow a + b(-L) = a + bL \Leftrightarrow b = 0
					\end{array}
					\right. \]

					Si $b = 0$ entonces la $a$ será arbitraria. $\Rightarrow \lambda_0 = 0$, $ X_0 \equiv a \in \mathbb{R}$


				\item $\lambda > 0$ con $\lambda = \mu^2$

					\[\begin{cases}
						X(x) = a\cdot e^{\mu x} + b \cdot e^{-\mu x} \\
						X'(x) = a\mu \cdot e^{\mu x} - b \mu \cdot e^{-\mu x}
					\end{cases}
					\Rightarrow a = b = 0 \]


				\item $\lambda < 0$ con $\lambda = -\mu^2$
					\[ X (x) = a \cos (\mu x) + b \sin (\mu x) \]

					\[\text{2 caminos: }\left\{\begin{array}{l}
						\text{ pensar }\rightarrow \mu = \frac{2\pi}{L} \\
						\text{ hacer cuentas }\footnotemark\rightarrow \left\{ \begin{array}{l}
							X(-L) = X(L) \\
							X'(-L) = X'(L)
						\end{array} \right.
					\end{array}
					\right. \]\footnotetext{se dejan como ejercicio}
			\end{itemize}

			Ajustamos $\mu$ para que $X$ sea periódica con periodo $2L k \Rightarrow … \Rightarrow \mu = \frac{k \pi}{L}$. Donde $2L$ es la longitud del intervalo. Por lo que llegamos a las soluciones:

			\[
			\lambda_k = -\left(\frac{k\pi}{L}\right)^2\quad\quad X_k(x) = a_k \cos \left( \frac{k\pi}{L} x \right) + b_k \sin \left( \frac{k\pi}{L} x \right)
			\]

			Resolvemos la EDO para $T$:
			\[ \lambda_0 = 0 \rightarrow T''=0 \rightarrow T(t) = \alpha_0 + \beta_0 t\]

			\[ \lambda_k = - \left( \frac{k\pi}{L} \right)^2, \quad T''_k = - \left( \frac{k\pi}{L} \right)^2 T_k \]

			\[\Rightarrow T_k(t) = \alpha_k \cos \left( \frac{k\pi}{L} t \right) + \beta_k \sin \left( \frac{k\pi}{L} t \right) \]

			Soluciones particulares de la EDO para T:

			\[u_k (x,t) = T_k(t) \cdot X_k(x) \quad k = 0,1,2,… \]


			Soluciones en forma de serie:
			\[u(x,t) = \sum_{k=1}^{\infty} u_k (x,t) \rightarrow \text{ Ajustar datos } f,g \]

			¿Converge esta serie?. La ecuación del calor era buena, ya que al tener una exponencial menor que 0 tendía a 0 muy rápido. En la ecuación de ondas la parte temporal no ayuda ya que tiene un comportamiento cualitativo distinto.

			Volvemos a nuestras soluciones particulares:

			\[u_0(x,t) = a_0 + b_0t\]

			% lo siento mucho si hay una errata en estas fórmula y te toca editarlas.

			\begin{align*}
			u_k(x,t) = &\left(\alpha_k \cos \left( \frac{k \pi}{L} t \right) + \beta_k \sin \left( \frac{k \pi}{L} t \right) \right) \\
			\cdot &\left(a_k \cos \left( \frac{k \pi}{L} x \right) + b_k \sin \left( \frac{k \pi}{L} x \right) \right)
			\end{align*}
			\begin{align*}
			u_k(x,t) = &\left(A_k \cos \left( \frac{k \pi}{L} x \right) + B_k \sin \left( \frac{k \pi}{L} x \right) \right) \cos \left( \frac{k \pi}{L} t \right)\\
			+ &\left(C_k \cos \left( \frac{k \pi}{L} x \right) + D_k \sin \left( \frac{k \pi}{L} x \right) \right) \sin \left( \frac{k \pi}{L} t \right)
			\end{align*}




			\[u_k(x,t) = T_k(t) X_k(t) \quad (k=0,1,2,…)\]


			y soluciones en forma de serie:

			\[ u(x,t) = u_0 + \sum_k u_k \]

			\begin{align*}
				u(x,t) = A_0 + C_0 t &+ \sum^{\infty}_{k=1} \left(A_k \cos \left( \frac{k \pi}{L} x \right) + B_k \sin \left( \frac{k \pi}{L} x \right) \right) \cos \left( \frac{k \pi}{L} t \right)\\
				&+ \sum^{\infty}_{k=1} \left(C_k \cos \left( \frac{k \pi}{L} x \right) + D_k \sin \left( \frac{k \pi}{L} x \right) \right) \sin \left( \frac{k \pi}{L} t \right)
			\end{align*}

			Evaluando en $t = 0$ obtenemos que
			\[f(x) = u(x,0) = A_0 + \sum_{k=1}^{\infty} A_k \cos \left( \frac{k \pi}{L} x \right) + B_k \sin \left( \frac{k \pi}{L} x \right) \]

			\[g(x) = u_t (x,0) = C_0 + \sum_{k=1}^{\infty} \left(\frac{k \pi}{L}\right) \left( C_k \cos \left( \frac{k \pi}{L} x \right) + D_k \sin \left( \frac{k \pi}{L} x \right)\right) \]


		\end{example}

		De 4 ejemplos hemos obtenido la misma solución:

		\[ f(x) = \sum_{k=1}^{\infty} b_k \sin \left( \frac{k \pi}{L} x \right), \quad x \in (0,L) \]

		\[ f(x) = a_0 + \sum^{\infty}_{k=1} a_k \cos \left( \frac{k \pi}{L} x \right), \quad x \in (0,L) \]

		\(
		f(x) \qeq a_0 + \sum^{\infty}_{k=1} a_k \cos \left( \frac{k \pi}{L} x \right) + b_k \sin \left( \frac{k \pi}{L} x \right), \quad x \in (-L,L)
		\)

		Ésta última es combinación de las dos anteriores, y nos lleva al problema que queremos resolver. ¿Es una solución?, y en ese caso: ¿Es única?.\\

		\textbf{Recordemos} las ecuaciones del coseno y seno suma:\index{Identidades trigonométricas}
		\[
		\left. \begin{array}{r}
			\cos (a + b) = \cos a \cos b - \sin a \sin b \\
			\cos (a - b) = \cos a \cos b + \sin a \sin b \\
		\end{array} \right\} \Rightarrow \left\{ \begin{array}{l}
			\cos a \cos b = \frac{1}{2} (\cos (a+b) + \cos (a-b)) \\
			\sin a \sin b = \frac{1}{2} (\cos (a-b) - \cos (a+b))
		\end{array} \right.
		\]

		\[
		\left. \begin{array}{r}
			\sin (a + b) = \sin a \cos b + \cos a \sin b \\
			\sin (a - b) = \sin a \cos b - \cos a \sin b \\
		\end{array} \right\} \Rightarrow \left\{ \begin{array}{l}
			\sin a \cos b = \frac{1}{2} \{\sin (a+b) + \sin(a-b)\}
		\end{array} \right.
		\]

		\begin{figure}[thbp]
		\centering
		\inputtikz{interpretacionDalembert}
		\caption{Interpretación de la ecuación D'Alembert}
		\label{fig:interpretacionDalembert}
		\end{figure}

		Con esto simplificamos (asumiendo ciertas regularidades de las integrales) las expresiones que obteníamos anteriormente y llegamos a la \concept[Fórmula\IS de D'Alembert]{Fórmula de D'Alembert}:

		\( u(x,t) = \frac{1}{2} \left\{ f(x+t) + f(x-t) \right\} + \frac{1}{2} \int^{x+t}_{x-t} g(s) \dif s  \label{eq:DALEMBERT} \)

		Su interpretación puede verse en la \fref{fig:interpretacionDalembert}

		Supongamos $g=0$ y $u(x,t) = \frac{1}{2} \{ f(x+t) - f(x-t)\} $


	\section{Nociones sobre convergencia}

	El problema que se abre ante nosotros es calcular el límite de la siguiente suma
	\[ \sum^{\infty}_{k=1} \Phi_k(x) \qeq f(x) \]

	Por simplificar, generalizaremos un poco: dada una sucesión $f_n$ de funciones\footnote{Para nosotros, lo que tendremos es $f_n = \sum\limits_{k=1}^n Φ_k(x)$, pero así es más sencillo de trabajar.}, ¿qué podemos decir del siguiente límite?
	\[ \lim_{n \rightarrow \infty} f_n(x) \eqexpl{?} f(x) \]

	Para tratar este tema con propiedad, vamos a ver primero algunas nociones sobre convergencia de funciones.

	\subsection{Convergencia puntual}

		La idea más sencilla de convergencia es la puntual: queremos ver que las funciones se acercan una a la otra punto a punto.

		Más formalmente, asumiendo que la sucesión de funciones $f_n$ están definidas en $(a,b)$, fijamos un $x_0 \in (a,b)$ y consideramos la sucesión de números $\set{f_n(x_0)}_{n ∈ ℤ} ⊂ ℝ$. Querremos ver entonces que
		\[ \liminft{n} f_n(x_0) = f(x_0) \quad \forall x_0 \in (a,b)\]

		Esto nos lleva a nuestra primera noción de convergencia.

		\begin{defn}[Convergencia\IS puntual] \label{def:ConvergenciaPuntual} Dada $\set{f_n}_{n ∈ ℤ}$ una sucesión de funciones, se dice que $f_n$ converge puntualmente a $f$ ($f_n \rightarrow f $ puntualmente) en $(a,b)$ si y sólo si para todo $x \in (a,b)$, dado $\epsilon > 0$ podemos encontrar $n_0$, tal que si $n \geq n_0$, entonces $|f_n(x) - f(x)| < \epsilon$.

			\begin{obs}
				$n_0$ es una función de $\epsilon$ y $x$: $n_0 = n_0(\epsilon, x)$
			\end{obs}

		\end{defn}

		Recordemos que la convergencia puntual no se comporta ``bien'' ni con la continuidad ni con las integrales.

		\begin{example}[De continuidad]
		Nuestra sucesión de funciones continuas será la siguiente:
			\[
			f_n(x) = \begin{cases}
			1 - nx & x \in [0, \frac{1}{n}]\\
			0 & x \in [\frac{1}{n},1]
			\end{cases} \quad \text{ continuo }\forall n
			\]
			que se puede ver que converge a lo siguiente:
			\[
			\lim_{n \rightarrow \infty} f_n(x) = \begin{cases}
			1 & x = 0\\
			0 & x \neq 0
			\end{cases}
			\]
			que es obviamente no continuo, como se ilustra en la \fref{fig:ejemploContinuidadPuntual}.

			\begin{figure}[thbp]
			\centering
			\begin{tikzpicture}[scale = 2]
				\draw[->] (-0.2,0) -- (1.5,0);
				\draw[->] (0,-0.2) -- (0,1.3);

				\draw[red, thick] (0,1) node [black, left] {1} -- (0.5,0) node [black, below] {$\frac{1}{n}$} -- (1,0) node [black, below] {$1$};

				\begin{scope}[xshift = 2.5cm]
				\draw[->] (-0.2,0) -- (1.5,0);
				\draw[->] (0,-0.2) -- (0,1.3);

				\draw[red, thick] (0,1) node [black, left] {1} .. controls (0.1,1.2) and (0.3,0.05) .. (0.7,0) node [black, below] {$\frac{1}{n}$} -- (1,0) node [black, below] {$1$};
				\end{scope}
			\end{tikzpicture}
			\caption{Una muestra de que la sucesión de funciones continuas (incluso $C^∞$, como en el caso de la derecha) da como límite una función no continua.}
			\label{fig:ejemploContinuidadPuntual}
			\end{figure}
		\end{example}

		\begin{example}[De integral]

			\begin{wrapfigure}{R}{0.4\textwidth}
			\centering
			\begin{tikzpicture}[scale = 2]
				\draw[->] (-0.2,0) -- (1.5,0);
				\draw[->] (0,-0.2) -- (0,1.3) node [left] {$f(x)$};

				\draw[red, thick] (0,0) -- (0.5,1) node (T) {} -- (1,0) node [black, below] {$\frac{1}{n}$};

				\draw[dashed] (0, 1) -- (0.5, 1);
				\node[hnlin, label={left:$n$}] at (0,1) {};
			\end{tikzpicture}
			\caption{La función tiene área constante, aunque converge puntualmente a $0$.}
			\label{fig:ejemploContinuidadPuntual2}
			\end{wrapfigure}

			Tomando una función como en la \fref{fig:ejemploContinuidadPuntual2}, vemos que converge puntualmente a $0$ para cualquier $x$, pero \[ \int_0^1 f_n(x) \dif x = \frac{1}{2} \quad \forall n \] y nos queda lo siguiente: \[ \lim_{n \to \infty} \int_0^1 f_n(x) \dif x = \frac{1}{2} \neq \int_0^1 \lim_{n \to \infty} f_n(x)  \dif x = 0  \]

		\end{example}

	\subsection{Convergencia uniforme}

		\begin{figure}[thbp]
		\centering
		\inputtikz{ConvergenciaUniforme}
		\caption{Interpretación geométrica de la convergencia uniforme}
		\label{fig:convergenciaUniforme}
		\end{figure}

		La sección anterior nos da una justificación clara de que necesitamos algo más que sólo convergencia puntual: tenemos que controlar de alguna forma todos los puntos a la vez para evitar problemas con la integral y con la continuidad. Este control será el de la convergencia uniforme.

		\begin{defn}[Convergencia\IS uniforme] \label{def:ConvergenciaUniforme} $\{f_n\}$ converge a $f$ uniformemente en $(a,b) \Leftrightarrow$ para todo $\epsilon > 0$, podemos encontrar ${n_0 = n_0(\epsilon)}$ tal que si $n \geq n_0$ entonces $|f_n(x) - f(x)| < \epsilon$, $\forall x \in (a,b)$.

		Esta definición es equivalente a decir que
		\[
			\sup_{x \in (a,b)} |f_n(x) - f(x)| < \epsilon
		\iff
			\left.||f_n - f||_{\infty}\right|_{(a,b)} < \epsilon
		\]
		\end{defn}

		Esta definición tiene una \textbf{interpretación geométrica} fácil de ver (\fref{fig:convergenciaUniforme}). Tenemos una banda de tamaño $2\epsilon$ alrededor de toda la gráfica de $f$, y si $n \geq n_0$, {\bf toda} la gráfica de $f_n$ está dentro de dicha banda.

		Como era de esperar, la convergencia uniforme funciona bien con la continuidad y las integrales.

		\begin{theorem} \label{thm:ConvergenciaUniforme}

			Supongamos que $f_n \rightarrow f$ uniformemente en $(a,b)$.

			\begin{itemize}
				\item $f_n$ continuas en $(a,b) \Rightarrow f$ continua en $(a,b)$

				\item $f_n$ integrables, $(a,b)$ acotado

				\[\int^{b}_{a} f(x) \dif x = \lim_{n \rightarrow \infty} \int^{b}_{a} f_n(x) \dif x \]
			\end{itemize}

			\obs La convergencia uniforme no preserva la derivabilidad

		\end{theorem}

		\begin{example}{\bf Valor absoluto}

			$\{f_n\}$ funciones ``redondeadas'' que convergen al valor absoluto. Ver \fref{fig:convergenciaValorAbsoluto}

			\begin{figure}[thbp]
			\centering
			\inputtikz{convergenciaValorAbsoluto}
			\caption{Ejemplo de la convergencia uniforme al valor absoluto con funciones derivables.}
			\label{fig:convergenciaValorAbsoluto}
			\end{figure}

			\[f_n(x) = \sqrt{x^2 + 1/n} \convs[][n][\infty] \sqrt{x^2} = |x| \]

		\end{example}

	\subsection{Convergencia en $L^2$}

		Aunque la convergencia uniforme parece que funciona bastante bien, es una convergencia muy fuerte y a veces no necesaria. Una convergencia ``intermedia'' entre la puntual y la uniforme será comprobar simplemente que la integral no se nos descogorcia por completo. Antes de entrar en detalle vamos a dar algunas definiciones.

		\newpage % FIXME: si se edita por encima, podría crear una página en blanco en medio del documento
		\begin{defn}[Espacio\IS de funciones $L^2$] \label{def:EspacioL2} El espacio $L^2((a,b))$ se define como el espacio de funciones integrables\footnote{Mejor dicho, medibles, pero no nos vamos a meter en ese percal en este curso. Para una introducción interesante a estos espacios, ver \citep{ApuntesVarReal}.} cuyo cuadrado de su norma tiene integral finita. Es decir, \[ L^2((a,b)) = \set{ \appl{f}{(a,b)}{ℝ} \tq \int_a^b \abs{f(x)}^2 \dif x < ∞ }\]
		\end{defn}

		\noindent Este espacio es un espacio vectorial, en el que hay una norma
		\( \norm{f}_2 = \left( \int^{b}_{a} |f|^2 \dif x \right)^{\frac{1}{2}} \label{eq:NormaL2} \)
		un producto escalar asociado (compatible con la norma)
		\( \pesc{f,g} = \int^b_a f(x) g(x) \dif x \label{eq:ProdEscalarL2} \)
		y, por supuesto, una noción de convergencia que es la que tenemos en cualquier espacio normado.

		\begin{defn}[Convergencia\IS en $L^2$] \label{def:ConvergenciaL2} Dadas $\set{f_n}_{n ∈ ℤ} ⊂ L^2$ y $f ∈ L^2$, se dice $f_n$ converge a $f$ en $L^2$ ($f_n \convs[L^2][n] f$) si y sólo si \[ \lim_{n\to ∞} \norm{f_n - f}_2 = 0\]
		\end{defn}

		Un resultado interesante en $L^2$ es que las funciones continuas son densas en él. Es decir, que si cogemos las funciones $C((a,b))$ y añadimos todos los límites de sucesiones de Cauchy, nos queda $L^2$. Esto nos será útil para el futuro, ya que nos permitirá decir que cualquier función $f ∈ L^2$ se puede aproximar como una sucesión de funciones $f_n$ continuas.

		Aunque no lo necesitaremos mucho, podemos definir la norma $L^p$ y su convergencia correspondiente.

		\begin{defn}[Convergencia\IS en $L^p$]
			\[L^p \rightarrow \|f\|_{p} = \left( \int^{b}_a |f|^p \dif x \right)^{\frac{1}{p}} \text{ para } 1 \leq p < \infty \]
		\end{defn}

		\begin{defn}[Convergencia\IS en $L^∞$] La norma infinito es el supremo, de tal forma que la convergencia en este caso es \[ f_n \convs[L^∞][n] f \iff \lim_{n\to ∞} \norm{f_n - f}_∞ = \lim_{n\to ∞} \sup_{x ∈ (a,b)} \abs{f_n(x) - f(x)} = 0\]
		\end{defn}

		\obs Si $p\neq 2$, en $L^p$ no hay producto escalar, y las cuentas se complican.
			Podría ser interesante estudiar para qué valores de $p$ las series de Fourier convergen, pero esto se sale del alcance de esta asignatura.

	% clase 2016/02/17

	\subsection{Relaciones entre las convergencias}

		Una vez que sabemos qué tipos de convergencias vamos a trabajar, nos interesará saber obviamente la relación entre ellas.

		Por ejemplo, es trivial ver que la convergencia uniforme implica convergencia puntual.

		\begin{wrapfigure}[12]{R}[0.15\textwidth]{0.4\textwidth}
		\vspace{-10pt}
		\inputtikz{ConvergenciaUniformeNoL2}
		\vspace{-10pt}
		\caption{Las funciones $f_n = \frac{1}{n} \ind_{[0, n]}$ convergen uniformemente a $0$ pero su integral siempre es $1$.}
		\label{fig:ConvergenciaUniformeNoL2}
		\end{wrapfigure}

		Además, la convergencia uniforme en conjuntos $(a,b)$ \textit{acotados} \textbf{implica} convergencia $L^2$. Para la demostración, simplemente vemos que con la convergencia uniforme $\norm{f_n - f}^2 < ε^2$ para $n$ mayor que un cierto $n_0$. Así, la integral será \[ \left( \int^{b}_a \abs{f_n-f}^2 \dif x \right)^{\frac{1}{2}} < \left( \int^{b}_a \epsilon^2 \dif x \right)^{\frac{1}{2}} = \epsilon \sqrt{b-a} \convs[ ][\epsilon][0] 0 \]

		En la ecuación anterior se puede ver que tendremos problemas en conjuntos no acotados, ya que entonces $b - a$ se nos puede ir a infinito, y en ese caso el límite y la integral no permutan. Un ejemplo de lo que puede ocurrir está en la \fref{fig:ConvergenciaUniformeNoL2}.



		El otro sentido no es cierto en general: la convergencia $L^2$ \textbf{no implica} convergencia uniforme. Usando el mismo ejemplo que en la \fref{fig:ejemploContinuidadPuntual}, tomamos ${f_n(x) = (1 -nx)\ind_{[0, \sfrac{1}{n}]}}$, de tal forma que integrando tenemos lo siguiente:
		\[ 0 \leq \int^1_0 |f_n|^2 \dif x \eqreason[\leq]{$0 \leq f_n \leq 1 \implies 0 \leq f_n^2 \leq f_n$} \int^1_0 |f_n| \dif x \eqreasonup{Área triángulo} \frac{1}{2n} \convs[ ][n] 0 \]

		En otras palabras, $f_n \convs[L^2] 0$, pero ya habíamos visto que esta función no converge uniformemente, ya que
		\[ \lim f_n =
		\begin{cases}
			1 & x = 0\\
			0 & x \neq 0
		\end{cases}\]

		La convergencia en $L^2$ \textbf{tampoco implica} convergencia puntual. Tomando la $f_n$ del ejemplo anterior, definimos $g_n = (-1)^n f_n$. La integral se va siempre a cero, luego $g_n \convs[L^2] 0$. Sin embargo, $g_n(0) = (-1)^n$, luego no hay convergencia puntual.

		El otro sentido también es falso en general: la convergencia puntual \textbf{no implica} convergencia en $L^2$. Con el mismo ejemplo visto en la \fref{fig:ejemploContinuidadPuntual2} (el triángulo cada vez más alto y estrecho):
		\[h_n^2 \convs 0 \text{ puntualmente, pero } \int_0^1 |h_n|^2 \dif x = \frac{1}{2}\quad \forall n \]

		De estas implicaciones y no implicaciones obtenemos los diagramas \ref{fig:diagramaConvergenciasAcotado} y \ref{fig:diagramaConvergenciasNoAcotado}.

		\begin{figure}[thbp]
		\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\inputtikz{diagramaConvergenciasAcotado}
		\caption{Implicaciones para un dominio acotado.}
		\label{fig:diagramaConvergenciasAcotado}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\inputtikz{diagramaConvergenciasNoAcotado}
		\caption{Implicaciones para un dominio no acotado.}
		\label{fig:diagramaConvergenciasNoAcotado}
		\end{subfigure}
		\caption{Implicaciones entre las distintas convergencias para dominios acotados y no acotados.}
		\label{fig:DiagramaConvergencias}
		\end{figure}

	\section{Series de Fourier}

		Una vez tratada la convergencia de funciones, recuperamos nuestro problema que era dar sentido a una expresión de la forma
		\[ f(x) = \frac{a_0}{2}+ \sum_{k ∈ ℕ} a_k \cos \left( \frac{k \pi}{L} x \right) + \sum_{k ∈ ℕ} b_k \sin \left( \frac{k \pi}{L} x \right) \]

		La interpretación que le podemos dar es que $\set{\frac{a_0}{2},a_k,b_k}_{k ∈ ℕ}$ son ``coordenadas''\footnote{Estamos hablando de un espacio vectorial con infinitas dimensiones, y todavía no hemos visto exactamente si ahí hay coordenadas} con respecto de la ``base'' dada por \[ \set{1, \cos \left( \frac{k \pi}{L} x \right), \sin \left( \frac{k \pi}{L} x \right) }_{k \in \nat } \]

		Si queremos seguir con esta interpretación, necesitaremos unas ciertas condiciones de ortogonalidad.

		\begin{theorem}[Condiciones de ortogonalidad] $ $ % hack para que el primer item no se "suba" hasta el título

			\begin{enumerate}[label=(\arabic*)]

				\item
				\[ \int^{L}_{-L} \cos \left( \frac{k \pi}{L} x \right) \dif x = \int^{L}_{-L} \sin \left( \frac{k \pi}{L} x \right) \dif x = 0, \forall k \]

				\[\Rightarrow \pesc{\cos \left( \frac{k \pi}{L} x \right),1}_{L^2} = \pesc{\sin \left( \frac{k \pi}{L} x \right),1}_{L_2} = 0\]

				\item
				\[ \int^{L}_{-L} \cos \left( \frac{k \pi}{L} x \right) \cdot \sin \left( \frac{j \pi}{L} x \right) \dif x = 0, \forall k, \forall j\]

				\[\Rightarrow \pesc{\cos \left( \frac{k \pi}{L} x \right), \sin \left( \frac{j \pi}{L} x \right) }_{L_2} = 0   \]

				\item
				\[ \int^{L}_{-L} \cos \left( \frac{k \pi}{L} x \right) \cdot \cos \left( \frac{j \pi}{L} x \right) \dif x = \begin{cases}
				0 & k \neq j \\
				L & k = j \end{cases} \]

				\[ \int^{L}_{-L} \sin \left( \frac{k \pi}{L} x \right) \cdot \sin \left( \frac{j \pi}{L} x \right) \dif x = \begin{cases}
				0 & k \neq j \\
				L & k = j \end{cases} \]

				\[\Rightarrow \pesc{\cos \left( \frac{k \pi}{L} x \right), \cos \left( \frac{j \pi}{L} x \right) }_{L_2} = \begin{cases}
				0 & k \neq j \\
				L & k = j \end{cases} \]

				\[ \pesc{\sin \left( \frac{k \pi}{L} x \right), \sin \left( \frac{j \pi}{L} x \right) }_{L_2} = \begin{cases}
				0 & k \neq j \\
				L & k = j \end{cases} \]

			\end{enumerate}
		\end{theorem}

			\begin{proof}

				\begin{itemize}
					\item Usar fórmulas trigonométricas

						\[\cos(A ± B), \sin(A ± B), …\]

					\item Usar complejos:

						\[ \cos(\theta) = \frac{e^{i\theta} + e^{-i\theta} }{2}, \quad \sin(\theta) = \frac{e^{i\theta} - e^{-i\theta} }{2i}\]

						\[  \int^{L}_{-L} \cos \left( \frac{k \pi}{L} x \right) \cdot \sin \left( \frac{j \pi}{L} x \right) dx = \]

						\[ = \int^L_{-L} \frac{e^{i\frac{k\pi}{L}x} + e^{-i\frac{k\pi}{L}x} }{2} \cdot \frac{e^{i\frac{j\pi}{L}x} - e^{-i\frac{j\pi}{L}x} }{2i} dx = … = 0 \]

				\end{itemize}

			\end{proof}


		% Aquí no he incluido una observación sobre un ejercicio en particular de la hoja


	\subsection{Cálculo de los coeficientes}

		La idea es que cada coeficiente es la proyección de $f$ en una dirección, de las infinitas direcciones que nos da la base.

		\textbf{Idea:} fijémonos en el caso de la suma finita

		\[ f(x) = \frac{a_0}{2}+ \sum_{k=1}^M a_k \cos \left( \frac{k \pi}{L} x \right) + \sum_{k=1}^M b_k \sin \left( \frac{k \pi}{L} x \right)\]

		Ahora fijamos $k=k_0$:

		\[ \int^L_{-L} f(x) \cos \left( \frac{k_0 \pi}{L} x \right) dx = \int^L_{-L} (\frac{a_0}{2} + \sum_k + \sum_k ) \cdot  \cos \left( \frac{k_0 \pi}{L} x \right) dx  = \]

		\[ \frac{a_0}{2}  \int^L_{-L} \cos \left( \frac{k_0 \pi}{L} x \right) dx +\\
		\sum_k^M a_k \int^L_{-L} \cos \left( \frac{k_0 \pi}{L} x \right) \cos \left( \frac{k \pi}{L} x \right) dx +
		\sum_k^M b_k \int^L_{-L} \cos \left( \frac{k_0 \pi}{L} x \right) \sin \left( \frac{k \pi}{L} x \right) dx \]

		Hemos cambiado de orden el sumatorio y la integral. En el caso de sumas finitas podemos hacerlo porque la integral es un operador lineal, pero debemos justificarlo para el caso de sumas infinitas.

		Volviendo a la cuenta que tenemos entre manos, tenemos que todas las integrales que estamos sumando son 0 menos una:

		\[ \dots = a_{k_0} \cdot L \longrightarrow a_{k_0} = \frac{1}{L} \int^{L}_{-L} f(x) \cos \left( \frac{k_0 \pi}{L} x \right) dx \]

		Análogamente:
		\[ b_{k_0} = \frac{1}{L} \int^L_{-L} f(x) \sin \left( \frac{k_0 \pi}{L} x \right) dx \]

		Quedaría por comprobar que $$a_0 = \frac{1}{L} \int^L_{-L} f(x) dx $$

		Pero tenemos un problema para justificar el cálculo cuando $M \rightarrow \infty$: permutar integral y suma infinita \[ \int \sum_1^{\infty} \qeq \sum_1^{\infty} \int\]

		Podemos definir:
		\begin{align}
		a_0 &= \frac{1}{L} \int_{-L}^L f(x) \dif x \nonumber \\
		a_k &= \frac{1}{L} \int_{-L}^L f(x) \cos \left( \frac{k \pi}{L} x \right) \dif x \nonumber  \\
		b_k &= \frac{1}{L} \int_{-L}^L f(x) \sin \left( \frac{k \pi}{L} x \right) \dif x \label{eq:CoefsFourier}
		\end{align}

		\textbf{Idea:} Dada $f(x)$ podemos calcular $a_0, a_k, b_k$ y construir:

		\[ f(x) \eqreason[≈]{¿En qué casos podremos poner un $=$?} \frac{a_0}{2} + \sum_{k=1}^{\infty} a_k \cos \left( \frac{k \pi}{L} x \right) dx + \sum_{k=1}^{\infty} b_k \sin \left( \frac{k \pi}{L} x \right) dx \]


		Veamos en que puntos la serie de la derecha converge. Pero necesitamos ver también que converge a la $f(x)$ de la que habíamos partido. No es trivial y solo se cumplirá en algunos casos.

		¿Por qué ocurre esto? La definición de límite tiene un inconveniente, que es necesario saber el límite para escribirlo. Cuando no lo sabemos, podemos usar el criterio de Cauchy para demostrar convergencia y demostrar que existe el límite. Desgraciadamente Cauchy no nos dará el valor del límite.

		Así, nuestra tarea a lo largo de la siguiente sección será ver cuándo y cómo converge esa serie de coeficientes a la función original.

	\subsection{Convergencia de la serie de Fourier}

		\subsubsection{Convergencia para funciones $C^2$}

		Vamos a empezar por lo fácil, pidiendo condiciones muy fuertes sobre las funciones. Empezaremos con funciones continuas y derivables dos veces, esto es, $f ∈ C^2$. En este caso, la herramienta a utilizar será el Criterio de Weierstrass.

		\begin{prop}[Criterio\IS de Weierstrass]\label{prop:criterio_Weierstrass} Supongamos que tenemos $\{\Phi_k\}$ sucesión de funciones en $(a,b)$, tales que para cada $k$, existe $M_k \in \real$ con $|\Phi_k(x)| \leq M_k$, $\forall x \in (a,b)$.

		Entonces: Si $\sum\limits_{k=1}^{\infty} M_k < \infty$, la serie $\sum\limits_{k=1}^{\infty} \Phi_k(x) $ converge uniformemente en $(a,b)$.
		\end{prop}

		\begin{proof}
			Queremos demostrar que la sucesión $\{\sum\limits_{k=1}^{N} \Phi_k(x)\}_{N=1,2,…} $ converge uniformemente en $(a,b)$. Para ello usaremos el criterio de Cauchy.\index{Criterio!de Cauchy} Basta probar que $\forall \epsilon > 0, \exists n_0 $ tal que si $n,m \geq n_0 $, entonces:

			\[ \abs{\sum^n_1 \Phi_k(x) - \sum^m_1 \Phi_k(x)} < \epsilon \quad \forall x \in (a,b) \]

			Suponiendo que $n > m$:
			\[ \abs{\sum^{n}_1 \Phi_k (x) - \sum^{m}_1 \Phi_k (x)} = \abs{\sum^n_{k=m+1} \Phi_k} \leq \sum^n_{k=m+1} \abs{\Phi_k (x)} \leq \sum_{m+1}^n M_k < \epsilon\]
			si $n,m$ son grandes por ser  $\sum\limits_{1}^{\infty} M_k < \infty $.
		\end{proof}

		Este criterio nos permitirá decir que la serie de Fourier converge uniformemente, aunque de momento no sepamos decir a qué.

		\begin{prop} \label{prop:ConvergenciaUniformeFourier} Sea $f \in C^2 ([-L, L])$ de período $2L$ (esto es, $f(-L) = f(L)$). Entonces, la serie de Fourier \[ \frac{a_0}{2} + \sum_{k=1}^\infty a_k \cos \left( \frac{k \pi}{L} x \right) + \sum_{k=1}^\infty b_k \sin \left( \frac{k \pi}{L} x \right) \] converge uniformemente en $[-L, L]$.
		\end{prop}

		\begin{proof} Vamos a tratar de aplicar el Criterio de Weierstrass, para lo que tendremos que acotar las funciones $a_k  \cos \left( \frac{k \pi}{L} x \right)$ y $b_k \sin \left( \frac{k \pi}{L} x \right)$ y demostrar que la suma de esas cotas es finita.

		La demostración es igual para ambas funciones, así que lo haremos sólo con la del coseno. Sabemos que $\abs{\cos x} ≤ 1$, luego sólo hay que acotar los coeficientes $a_k$, que estaban definidos de la siguiente forma: \[a_k = \frac{1}{L} \int_{-l}^L \cos \left( \frac{k \pi}{L} s \right) f(s) \dif s \]

		Lo que haremos será integrar por partes dos veces, aprovechando que podemos derivar $f ∈ C^2$:
		\begin{align*}
		a_k &= \frac{1}{L} \int_{-L}^L \cos \left( \frac{k \pi}{L} s \right) f(s) \dif s = \\
			&= \frac{1}{L} \left[
				\underbracket{\eval{f(s)
					\frac{\sin \left( \frac{k \pi}{L} s \right)}
						{\frac{k \pi}{L}}
				}_{s=-L}^{L}}_{= 0 \text{ ya que } \sin (\pm kπ) = 0}
			- \int^L_{-L} f'(s)
			  \frac{\sin \left(\frac{k \pi}{L} s \right)}{\frac{k \pi}{L}} \dif s
			\right] = \\
			&= \frac{1}{kπ} \left[ -\int^L_{-L} f'(s) \sin \left( \frac{k \pi}{L} s \right) \dif s \right] = \\
			&= \frac{1}{kπ} \left[
				\underbracket{\eval{f'(s)
					\frac{ \cos \left( \frac{k \pi}{L} s \right)}
						{\frac{k \pi}{L}}
				}_{s = -L}^L}_{= 0 \text{ ya que } \cos (kπ) = \cos (-kπ)}
				- \int^L_{-L} f''(s)
				\frac{ \cos \left(\frac{k \pi}{L} s \right)}{\frac{k \pi}{L}} \dif s
			\right] = \\
			&= \frac{-L}{k^2 π^2} \int_{-L}^{L} f''(s) \cos \left(\frac{k \pi}{L} s \right) \dif s
		\end{align*}

		Dado que $f ∈ C^2$, $f''$ está acotada por una constante $M$, y como $\abs{\cos x} ≤ 1$ entonces podemos acotar todo eso: \[ \abs{a_k} = \abs{\frac{L}{k^2 π^2} \int_{-L}^{L} f''(s) \cos \left(\frac{k \pi}{L} s \right) \dif s} ≤ \frac{L}{k^2π^2} \int_{-L}^L M \dif s = \frac{C}{k^2} \]

		Ahora podemos aplicar el criterio de Weierstrass: dado que
		$$\abs{a_k  \cos \left( \frac{k \pi}{L} x \right)} \leq \abs{a_k} \leq \frac{C}{k^2} \ \text{ en } [-L, L] \ \text{ y } \ \sum\limits_{k=1}^∞ \frac{C}{k^2} < ∞$$
		tenemos que la serie $\sum\limits_{k=1}^∞ a_k  \cos \left( \frac{k \pi}{L} x \right)$ converge uniformemente en $[-L, L]$.

		Realizando el proceso análogo con los coeficientes $b_k$ llegaríamos a que la otra parte también converge uniformemente, y por lo tanto toda la serie de Fourier converge uniformemente en $[-L, L]$.
		\end{proof}

		\noindent \textbf{Conclusión:} El criterio de Weierstrass nos implica convergencia uniforme en $[-L,L]$.

		\obs Si $f \in C^3$ (más las condiciones de periodicidad necesarias) podría hacer partes otra vez y obtener la cota $\frac{C}{k^3}$. Y así hasta $C^n$: cuanta mayor regularidad le pidamos a la $f$ (y las condiciones de periodicidad oportunas para las cancelaciones) mayor orden tendrá el $\frac{1}{k}$.

		% clase 2016/02/22

		Veamos también que hay un problema sutil. Dado $f$, calculamos los coeficientes $\{a_0,a_k,b_k\}$ y construimos una serie que hemos dicho que converge. Pero esa serie, ¿converge a $f$?

		\begin{lemma}
			Supongamos $\sum \Psi_k$ converge uniformemente en $(a,b)$ y sea $h(x)$ contínua.

			\begin{itemize}
				\item $\sum\limits_k h \Psi_k$ converge uniformemente en $(a,b)$ y además:
				\[ \sum_k h \Psi_k = h \sum_k \Psi_k\]

				\item $\displaystyle \int_a^b \sum_k h \Psi_k = \sum_k \int_a^b h \Psi_k$.
			\end{itemize}

		\end{lemma}

		Este lema justifica que los coeficientes del desarrollo trigonométrico de
		$$g(x) = a_0 + \sum a_k \cos \left( \frac{k \pi}{L} x \right) + \sum b_k \sin \left( \frac{k \pi}{L} x \right) $$
		son $\set{a_0,a_k,b_k}_{k \in \nat}$

		En resumidas cuentas, a lo que hemos llegado es a que dada $f \in C^2$ periódica, la serie converge uniformemente. Ahora bien, seguimos sin saber identificar el límite. Vamos a dejarlo para más adelante y tratar de identificar el límite con condiciones más débiles.

		\subsubsection{Convergencia en $L^2$}

		Como estamos en $L^2$, que es un espacio vectorial, trabajaremos con nuestra base $\set{1, \cos \left( \frac{k \pi}{L} x \right), \sin \left( \frac{k \pi}{L} x \right)}_{k \in \nat}$. Es ortogonal, así que vamos a calcular los módulos para normalizar:
		\begin{align*}
		\norm{1}_{L^2(-L,L)} &= \left( \int_{-L}^{L} 1^2 ds \right)^{1/2} = \sqrt{2L}\\
		\norm{\cos \left( \frac{k \pi}{L} x \right)} &= \left( \int_{-L}^{L} \cos^2 (\frac{k\pi s}{L}) ds \right)^{1/2} \eqreason{$\cos(2s) = 2 \cos^2 (s) - 1$} \left( \int_{-L}^{L} \frac{1}{2} + \frac{1}{2} \cos (\frac{2 k \pi s}{L}) ds \right)^{1/2} = \sqrt{L} \\
		\norm{\sin \left( \frac{k \pi}{L} x \right)} &= \dots = \sqrt{L}
		\end{align*}

		Con lo que llegamos a la base ortonormal \[ \set{\frac{1}{\sqrt{2L}}, \frac{1}{\sqrt{L}} \cos \left( \frac{k \pi}{L} x \right),\frac{1}{\sqrt{L}} \sin \left( \frac{k \pi}{L} x \right)}_{k \in \nat} \]

		Con esta base, la serie de Fourier no es más que la expresión de la función (o al menos, eso esperamos) en esa base concreta. Si recordamos la definición del produto escalar\footnote{$\displaystyle \pesc{f,g} = \int_{-L}^L fg \dif x$.} en $L^2$, tenemos \begin{align*}
		a_k \cos \left( \frac{k \pi}{L} x \right)
			&= \left(\frac{1}{L} \int^L_{-L} f(s) \cos \left( \frac{k \pi}{L} s \right) \dif s \right) \cos \left( \frac{k \pi}{L} x \right) =  \\
			&= \pesc{f, \frac{1}{\sqrt{L}} \cos \left( \frac{k \pi}{L} s \right)} \cdot \frac{1}{\sqrt{L}} \cos \left( \frac{k \pi}{L} x \right)
		\end{align*}

		Por comodidad, nos gustaría tener un sistema $\set{Φ_k}_{k ∈ ℕ}$ ortonormal, con $\pesc{\Phi_k,\Phi_j}_{L^2} = \delta_{kj}$, de tal forma que simplemente podamos escribir \[ f ≈ \sum_k \pesc{f, \Phi_k}_{L^2} \cdot \Phi_k (x) \]

		\noindent Para lograrlo, introducimos la \textbf{Notación compleja}.

		\noindent Sabiendo que $e^{i\theta} = \cos \theta + i \sin \theta$, calculamos:
		\begin{align*}
		a_k
			&= \frac{1}{L} \int^{L}_{-L} \cos \left( \frac{k \pi}{L} s \right) f(s) \dif s
		& \frac{a_k - ib_k}{2}
			&= \frac{1}{2L} \int_{-L}^L f(s) e^{-i \left( \frac{k \pi}{L} s \right)} \dif s \\
		b_k &= \frac{1}{L} \int^{L}_{-L} \sin \left( \frac{k \pi}{L} s \right) f(s) \dif s
		& \frac{a_k + ib_k}{2}
			&= \frac{1}{2L} \int_{-L}^L f(s) e^{+i \left( \frac{k \pi}{L} s \right)} \dif s \\
		\end{align*}
		y definimos:
		\[
			\alpha_k = \frac{a_k - ib_k}{2}, \quad \alpha_{-k} = \frac{a_k + ib_k}{2}
		\]

		Esto también da un sistema ortogonal, pero ¿cómo normalizamos la base $\set{ e^{i \left( \frac{k \pi}{L} s \right)}}_{k = 0,±1,±2,\dotsc}$?

		Para verlo, primero tenemos que definir bien el producto escalar cuando las funciones pueden ser complejas
		\(\pesc{f,g}_{L^2} = \int^L_{-L} f(x) \conj{g(x)} \dif x \label{eq:ProdEscalarComplejo} \)

		De esta forma, los coeficientes nos quedan con una expresión bastante amable:
		\[ \alpha_{\pm k} = \frac{1}{2L} \int^L_{-L} f(s) e^{-i \left( \frac{\pm k \pi}{L} s \right)} \dif s = \frac{1}{2L} \int_{-L}^L f(s) \conj{e^{i \left( \frac{\pm k \pi}{L} s \right)}} \dif s = \frac{1}{2L} \pesc{f, e^{i \left( \frac{\pm k \pi}{L} x \right)}} \]

		Sólo queda normalizar los elementos de la base:
		\[
		\left\|e^{i \left( \frac{k \pi}{L} x \right)}\right\|_{L^2} = \left( \pesc{ e^{i \left( \frac{k \pi}{L} x \right)}, e^{i \left( \frac{k \pi}{L} x \right)} }_{L^2} \right)^{\frac{1}{2}} = \left( \int^{L}_{-L} e^{i \left( \frac{k \pi}{L} x \right)} \cdot \conj{ e^{i \left( \frac{k \pi}{L} x \right)}} \dif x \right)^{\frac{1}{2}} = \sqrt{2L}
		\]

		Con lo que llegamos al sistema ortonormal:
		\[
			\left\{ \frac{1}{\sqrt{2L}} e^{i \left( \frac{k \pi}{L} x \right)} \right\}_{k = 0, ±1, ±2, …}
		\]
		y a la siguiente expresión para la serie de Fourier
		\[
		f ≈ \sum_{k=-\infty}^\infty \alpha_k e^{i \left( \frac{k \pi}{L} x \right)} = \sum_{k = -\infty}^\infty \pesc{ f, \frac{1}{\sqrt{2L}} e^{i \left( \frac{k \pi}{L} x \right)} } · \frac{e^{i \left( \frac{k \pi}{L} x \right)}}{\sqrt{2L}}
		\]

		Así, a partir de ahora denotaremos nuestros elementos de la base ortonormal como $Φ_k = \frac{e^{i \left( \frac{k \pi}{L} x \right)}}{\sqrt{2L}}$, con $\norm{Φ_k}_{L^2} = 1$ y podremos escribir simplemente
		\[ f ≈ \sum_{k∈ℤ}\pesc{f, \Phi_k} \cdot \Phi_k \]

		Para estudiar la convergencia de esa suma infinita, lo que haremos será lo habitual en estos casos: definir las sumas parciales y estudiar su límite. Así, definiremos
		\[
		S_n f = \sum_{|k| < n} \pesc{f, \Phi_k}
		\]

		Y ahora la pregunta del millón: ¿$\| f - S_n f \|_{L^2} \rightarrow 0$?

		Primero, vamos a estudiar la convergencia. Vamos a ver cómo se comporta la norma de las sumas parciales
		\begin{align*}
		\|S_n f\|_{L^2}^2 &= \pesc{S_n f, S_n f}_{L^2} = \sum_{|k| < n} \pesc{f, \Phi_k} \Phi_k \cdot \sum_{|j| < n} \pesc{f, \Phi_j} \Phi_j = \\
			&\eqreasonup{$\sum$ finitas} \sum_{k, j < \abs{n}} \pesc{f, \Phi_k} \pesc{f, \Phi_j}\underbrace{\pesc{\Phi_k, \Phi_j}}_{\delta_{kj}} = \sum_k \abs{\pesc{f, \Phi_k}}^2
		\end{align*}

		Luego concluimos que $\|S_n f\|_{L^2}^2$ es creciente. Vamos a ver ahora qué ocurre con $\norm{f - S_n f}^2 = \pesc{f - S_nf, f - S_nf}$. Primero vamos a calcular el producto $\pesc{f, S_n f}$:
		\[
			\pesc{f, S_n f} = \pesc{f, \sum_{|k| < n} \pesc{f, \Phi_k} \Phi_k} \eqreason{$\sum$ finita} \sum_{|k| < n} \abs{\pesc{f, \Phi_k}}^2
		\]

		Con esto ya podemos trabajar con la norma:
		\begin{align*}
		0 	&≤ \|f - S_n f\|^2_{L^2} = \pesc{f - S_n f, f- S_n f} = \\
			&= \pesc{f,f} - 2 \pesc{S_n f, f} + \pesc{S_n f, S_n f} = \\
			&= \|f\|^2_{L^2} - \sum_{|k| < n} \abs{\pesc{f, \Phi_k}}^2 \\
		 \sum_{|k| < N} \abs{\pesc{f, \Phi_k}}^2 &\leq \|f\|^2_{L^2}
		\end{align*}

		\obs Con funciones complejas, el producto escalar en $ℂ$ no es conmutativo: $\pesc{f,g} = \conj{\pesc{g,f}}$. Por suerte, aquí $\pesc{S_nf, f} ∈ ℝ$ siempre por lo que no tenemos problemas en hacer el intercambio en el cálculo anterior. Lo único que hay que tener en cuenta es que cuando trabajamos con complejos, el valor absoluto es simplemente el módulo.

		\obs  \[
			\left.
			\begin{array}{l}
				\|S_n f\|^2_2 \text{ creciente } \\
				\|S_n f\|_2 \text{ acotado superiormente }
			\end{array}
			\right\} \Rightarrow \|S_n f\|_{2} \text{ converge }
		\]

		Pero ¿$\|S_n f\|_2 \convs \| f \|_2$?. Estamos en el mismo punto.

		\textbf{Conclusión:} Si demostramos que  $\|S_n f\|_{L^2} \convs \| f \|_{L^2}$ entonces el último cálculo probaría que $S_n f \convs[ L^2 ] f$

		\obs De lo obtenido en el último cálculo tenemos que:
			\[ \sum_{|k| < n} |\pesc{f, \Phi_k}|^2 \leq \|f\|_2^2 \quad \forall n \]

			Haciendo tender a $n$ a infinito obtenemos la \concept{Desigualdad\IS de Bessel}:
			\( \sum_{k=-\infty}^\infty |\pesc{f, \Phi_k}|^2 \leq \|f\|_2^2 \label{eq:desigualdad_bessel}  \)

			En $\real^3$ esto equivale al teorema de Pitágoras, pero nos falta ver el $\geq$.

			Lo que sí nos permite ver esa desigualdad es que si $f \in L^2$, entonces la norma del producto escalar de $f$ por los elementos de la base $\Phi_k$ tiende a 0. Esto nos permite dar  una versión del lema de Riemann-Lebesgue:

	% clase 2016/02/23

			\begin{lemma}[Lema\IS de Riemman-Lebesgue] \label{lem:RiemannLebesgue}
			$ $ %hack

			\noindent Sea $f ∈ L^2$ y $\set{Φ_k}_{k ∈ ℕ}$ una base ortonormal de $L^2$. Entonces, las ``coordenadas'' de $f$ se van a cero: \[ \abs{\pesc{f, \Phi_k}} \convs[ ][k][\infty] 0 \]
			\end{lemma}

			Y damos un pequeño ejemplo de su aplicación:
				\[ \int_{-L}^{L} f(s) \cdot \cos\left(\frac{k\pi}{L}s\right) \dif s \convs[ ][k][\infty] 0 \]


	\subsubsection{Resumen}

		\subsubsection*{Convergencia uniforme}

			\begin{itemize}[itemsep = 0pt]
				\item Sistema trigonométrico (o exponencial complejo).
				\item $f \in C^2$ periódica
				\item No identificamos el límite
				\item Los coeficientes decaen en función de la regularidad de f
			\end{itemize}

		\subsubsection*{Convergencia $L^2$}

			\begin{itemize}[itemsep = 0pt]
				\item Resultados válidos para cualquier sistema ortonormal $\{ \Phi_k \}$ y  $f \in L^2$.
				\item No identificamos el límite: falta probar $\|S_N f\|_2 \convs \|f\|_2$.
				\item Hay que utilizar la Desigualdad de Bessel (la \fref{eq:desigualdad_bessel}) y el Lema de Riemman-Lebesgue (\fref{lem:RiemannLebesgue}).
			\end{itemize}

		\subsection{Identificación del límite con convergencia uniforme}
		\label{sec:IdentificacionLimite}

		Tenemos un problema con estas convergencias en cuanto a que ninguna nos permite identificar el límite. Vamos a dar un pequeño rodeo para llegar a demostrar lo que queremos.

		Comencemos con un ejemplo, planteando otro \textbf{problema distinto:}
		\[
			\begin{cases}
				u_{xx} + u_{yy} = 0 & \text{ en } x^2 + y^2 < 1 \\
				u(x,y) = f(x,y) & \text{ en } x^2 + y^2 = 1
			\end{cases}
		\]
		con $f$ tan regular como necesitemos.

		Hacemos el cambio a coordenadas polares:
		\begin{align*}
		 r^2 = x^2 + y^2  &\eqexpl[\longrightarrow]{derivando} rr_x = x , \quad r_x = \frac{x}{r} \eqreasonup{$x = r \cos \theta $} \cos \theta \\
		 \tan \theta = \frac{y}{x}  &\eqexpl[\longrightarrow]{derivando} \frac{1}{\cos^2 \theta} \theta_x = \frac{-y}{x^2}, \quad \theta_x = \frac{-y \cos^2\theta}{x^2} \eqreason{$y = r \sin \theta, x = r \cos \theta$} \frac{-r \sin \theta \cos^2 \theta}{r^2 \cos^2 \theta} = \frac{- \sin \theta}{r}
		\end{align*}
		Tenemos:
		\begin{align*}
			W(r, \theta) &= u(r \cos \theta, r \sin \theta)\\
			u_x &= W_r \cdot r_x + W_\theta \cdot \theta_x
		\end{align*}
		Sustituyendo $r_x$ y $\theta_x$ en $u_x$:
		\begin{align*}
			u_x = W_r \cdot \cos \theta + W_\theta \cdot \frac{- \sin \theta}{r}
		\end{align*}
		Ahora podemos calcular $u_{xx}$:
		$$ u_{xx} = (W_r \cdot \cos \theta + W_\theta \cdot \frac{- \sin \theta}{r})_x \ r_x + (W_r \cdot \cos \theta + W_\theta \cdot \frac{- \sin \theta}{r})_\theta \ \theta_x$$

		Se deja como ejercicio para el lector el resto de cuentas y repetir el procedimiento para obtener $u_y$ y $u_{yy}$. Al final obtenemos:
		$$ u_{xx} + u_{yy} = 0 = \dots = W_{rr} + \frac{1}{r}W_r + \frac{1}{r^2}W_{\theta\theta}$$
		Con \textbf{dato}:
		\[ u(x,y) = f(x,y), \text{ si } x^2 + y^2 = 1\]
		Luego
		\[ \underbrace{u(1 \cdot \cos \theta,1 \cdot \sin \theta)}_{W(1,\theta)} = f(\cos \theta, \sin \theta) \equiv g(\theta)\]

		Queremos resolver:
		\[
			\begin{cases}
			W_{rr} + \frac{1}{r} W_r + \frac{1}{r^2} W_{\theta \theta} = 0 & 0 < r < 1 \\
			W(1, \theta) = g(\theta) & \theta \in (-\pi,\pi)
			\end{cases}
		\]

		\begin{itemize}
			\item Necesitamos que $W$ sea continua en $r=0$
			\item $W$ es $2\pi$-periódica en $\theta$
		\end{itemize}

		\noindent Aplicamos el método de separación de variables, es decir, buscamos $$W(r, \theta) = R(r)\cdot \Phi(\theta)$$
		Sustituimos en el sistema:
		\[R''\Phi + \frac{R'}{r}\Phi + \frac{R}{r^2} \Phi'' = 0 \]
		Y operamos
		\[ - \left( R'' \Phi + \frac{R'}{r}\Phi \right) = \frac{R}{r^2} \Phi'' \Rightarrow \underbrace{\frac{(r^2R''+rR')}{R}}_{\text{Función de }r} = \underbrace{\frac{\Phi''}{\Phi}}_{\text{Función de }\theta} = \lambda \in \real \]

		Luego necesitamos que
		\begin{itemize}
			\item $R$ continua en 0
			\item $\Phi$ $2\pi$-periódica
		\end{itemize}


		Resolvemos la EDO en $\Phi$ (Ecuación de ondas)
		\[
			\begin{cases}
				\Phi'' = \lambda \Phi & \theta \in (-\pi, \pi) \\
				\Phi & 2\pi \text{ periódica }
			\end{cases}
		\]

		Como esta es la ecuación de ondas \ref{ec:ondas}, podemos plantar la solución:
		\[
			\begin{cases}
				\lambda_0 = 0,& \Phi_0 = c \in \real\\
				\lambda_k = -(\frac{\pi k}{L})^2 \eqreason{$\pi=L$} -k^2, & \Phi_k = A_k \cos (k\theta) + B_k \sin (k\theta)\\
				\lambda_k > 0 & \text{NO hay solución}
			\end{cases}
		\]

		Volvemos a la EDO en $R$ para ver sus soluciones a partir de las de $\Phi$:
		\begin{align*}
			\lambda_0 = 0 \implies & r\cdot R_0'' + R'_0 = 0\\
			& r\cdot R_0'' = - R'_0\\
			& \frac{R''_0}{R'_0} = \frac{-1}{r} \quad \text{ (integramos) }\\
			& \ln(R'_0) = - \ln(r) + C \quad \text{ (exponenciamos) }\\
			& R'_0 = \frac{A}{r}, \quad A \in \real \text{ (integramos) }\\
			& R_0 = A \cdot \ln(r) + B, \quad A,B \in \real
		\end{align*}
		Como $R_0(r)$ debe estar acotada y continua en 0, tenemos que $A = 0$, luego $$W_{0}(r, \theta) = A_0$$

		Vamos a ver qué ocurre con $\lambda_k = -k^2$:
		\[r^2 R''_k + r R'_k - k^2 R_k = 0 \quad \text{\bf Ecuación de Euler}\]\index{Ecuación! de Euler}

		Recordemos cómo se resuelve: buscamos soluciones de la forma
		\begin{align*}
			R_k &= r^\alpha \\
			R'_k &= \alpha r^{\alpha - 1} \\
			R''_k &= \alpha(\alpha-1)r^{\alpha - 2}
		\end{align*}
		Sustituimos en la ecuación original:
		\[ 0 = r^\alpha [\alpha(\alpha -1)+ \alpha - k^2] = r^\alpha [\alpha^2 - k^2] \]
		Y obtenemos que las raíces del polinomio característico son $\alpha = ± k$.

		Luego la solución quedaría:
		\[ R_k(r) = \alpha_k r^k + \beta_k r^{-k} \]
		Donde $\beta_k r^{-k}$ tiende a 0 porque buscamos soluciones continuas, luego acotadas.

		Finalmente, obtenemos que
		$$W_k(r, \theta) = r^k (A_k \cos(k\cdot \theta) + B_k \sin(k\cdot \theta))$$

		Luego nuestro {\bf candidato a solución}:
		\[\frac{A_0}{2} + \sum_{k=1} r^k \{A_k \cos(k \theta) + B_k \sin(k\theta)\}   \]

		Dato: $W(1,\theta) = g(\theta)$.

		Observamos que esta solución es buena porque si ${r<1}$, por el criterio de Weierstrass (\fref{prop:criterio_Weierstrass}), converge uniformemente en $[-\pi, \pi]$.

		Necesitamos:
		\[ g(\theta) \qeq \frac{x_0}{2} + \sum_{k = 1}^{\infty} A_k \cos(k \theta) + B_k \sin(k \theta) \]

		Debe ser entonces:
		\begin{align*}
			A_0 &= \frac{1}{\pi} \int_{-\pi}^{\pi} g(s) ds \\
			A_k &= \frac{1}{\pi} \int_{-\pi}^{\pi} g(s)\cos(ks) ds \\
			B_k &= \frac{1}{\pi} \int_{-\pi}^{\pi} g(s) \sin(ks) ds
		\end{align*}

		\begin{obs}
			\[ |A_0|, |A_k|, |B_k| < \frac{1}{\pi} \int_{-\pi}^{\pi} |g(s)| ds \]

			No necesitamos propiedades de regularidad, nos basta con que $g$ sea integrable.
		\end{obs}

		Podemos definir:

		\( W(r,\theta) = \frac{A_0}{2} + \sum_k r^k(a_k \cos(k \theta) + B_k \sin(k\theta)) \label{eq:serieW} \)

		Pero llegados aquí, nos preguntamos:

		\begin{itemize}
			\item ¿$W$ es solución?
			\item ¿$W(1, \theta) = g(\theta)$?
		\end{itemize}

		La suerte que tenemos es que es una serie geométrica en $\cplex$, que con $r<1$ no solo converge, sino que podemos escribir la solución.

		Pasamos a notación compleja:
		\begin{align*}
			\alpha_k &= \frac{1}{2\pi} \int_{-\pi}^{\pi} g(s) e^{-iks} ds \\
			\alpha_k &= \frac{A_k - iB_k}{2} \\
			\alpha_{-k} &= \frac{A_k + iB_k}{2}
		\end{align*}

		Podemos reescribir \eqref{eq:serieW} entonces como:

		\begin{align*}
		W(r, \theta) &= \alpha_0 + \sum_{k=1}^{\infty} r^k \left( \alpha_k\  e^{ik\theta} + \alpha_{-k}\  e^{-ik\theta} \right)\\
%
		&= \sum_{k=0}^{\infty} \alpha_k\  r^k e^{ik\theta} + \sum_{l=1}^{\infty} \alpha_{-l}\ r^l e^{-il\theta} \quad (\text{Cambio: } k = -l)\\
%
		&= \sum_{k=0}^{\infty} \alpha_k\  r^k e^{ik\theta} + \sum_{k=-\infty}^{-1} \alpha_{k}\  r^{\abs{k}} e^{ik\theta} = \sum_{k=-\infty}^{\infty} \alpha_{k}\  r^{\abs{k}} e^{ik\theta}\\
%
		&= \sum_{k=-\infty}^{\infty} \underbrace{\left( \frac{1}{2\pi} \int\limits_{-\pi}^{\pi} g(s) e^{-iks} ds \right)}_{\leq \frac{1}{2\pi} \int_{-\pi}^{\pi} |g(s)| ds < C} r ^{|k|} \underbrace{e^{ik\theta}}_{|e^{ik\theta}| \leq 1}\\
		\end{align*}
		Donde hemos aplicado el criterio de Weierstrass \ref{prop:criterio_Weierstrass}.

		% Clase 2016/02/24
		Si $r < 1$, tenemos que $\sum\limits_k Cr^{|k|} < \infty$, luego tenemos convergencia uniforme de la serie $\mathlarger{\sum}\limits_{k=-\infty}^{\infty} \alpha_{k} r^{\abs{k}} e^{ik\theta}$ y podemos intercambiar límite e integral.

		Por tanto, nuestra última ecuación es igual a:
		\[ W(r,\theta) = \dots = \frac{1}{2\pi} \int_{-\pi}^{\pi} g(s) \sum_{k=-\infty}^{\infty} r^{|k|} e^{ik(\theta-s)} ds \]

		Como $$\sum\limits_{k=-\infty}^{\infty} = \sum\limits_{k=0}^{\infty} + \sum\limits_{k=-1}^{-\infty}$$

		Por lo que obtenemos:
		\[
		\sum_{k=0}^{\infty} (\underbrace{r e^{i (\theta-s)}}_{|re^{i(\theta-s)}| < r < 1})^k \eqreasonup{\text{serie geométrica}} \frac{1}{1-re^{i(\theta-s)}}
		\]

		Realizamos el mismo proceso con la otra suma:
		\begin{gather*}
			\sum\limits_{-1}^{-\infty} r^{\abs{k}} e^{ik(\theta-s)} = \sum\limits_{-1}^{-\infty} r^{-k} e^{ik(\theta-s)} \eqreasonup{$k=-l$} \ \sum\limits_{1}^{\infty} r^{l} e^{-il(\theta-s)} = \sum\limits_{1}^{\infty} (r \cdot e^{-i(\theta-s)})^l = \frac{r \cdot e^{-i(\theta-s)}}{1-r \cdot e^{-i(\theta-s)}}
		\end{gather*}

		Y llegamos a la conclusión:
		\[
		W(r,\theta) = \frac{1}{2\pi} \int_{-\pi}^\pi g(s) \sum_{k=-\infty}^\infty r^{|k|} e^{ik(\theta-s)} = \frac{1}{2\pi} \int_{-\pi}^\pi g(s) \left\{ \frac{1}{1-re^{i(\theta-s)}} + \frac{re^{-i(\theta-s)}}{1-re^{-i(\theta-s)}} \right\} ds	\]

		Ahora usando números complejos, llegamos a la \concept{Integral\IS de Poisson de g}
		\[ = \int^{\pi}_{-\pi} g(s) \underbrace{\frac{1}{2\pi} \frac{1-r^2}{1+r^2-2r\cos (\theta - s)}}_{\text{\bf Núcleo de Poisson}} ds \]

		Ahora veremos la relación entre las propiedades de la serie y las propiedades del \concept{Núcleo\IS de Poisson}, que es:
		\( P(r,\alpha) = \frac{1}{2\pi} \frac{1-r^2}{1+r^2-2r\cos (\alpha)} \label{eq:NucleoPoisson} \)

		\begin{prop}[Propiedades\IS del núcleo de Poisson]
		$ $ % hack

		\begin{itemize}

			\item Fijo $r \in \left[ 0,1 \right)$, $P(r,\alpha) = P(r,-\alpha)$, $2\pi$-periódico en $\alpha$.

			\item $P(r,\alpha) > 0$

			\item $\dpd{P}{\alpha} < 0$ en $\alpha \in \left[ 0,\pi \right)$

			\[ \text{En particular} \begin{cases}
				\max\limits_{\alpha \in [\delta, \pi)} P(r,\alpha) = P(r,\delta), \quad \delta>0\\
				\min P(r,\alpha) = P(r,\pi) = P(r,-\pi)
			\end{cases} \]

			\obs \[ P(r, \theta) = \frac{1}{2\pi}\frac{1-r^2}{1+r^2-2r \cos(\theta)} = \frac{1}{2\pi} \sum_{-\infty}^\infty r^{|k|} e^{i k \alpha} \]

			\item \[
				\int_{-\pi}^\pi P(r, \alpha) d\alpha = 1 \quad \forall r < 1
			\]

			\begin{proof}
				Utilizar \[ \int_{-\pi}^\pi P(r, \alpha) = \frac{1}{2\pi} \int_{-\pi}^\pi \sum_{k=-\infty}^{\infty} r^{|k|} \cdot e^{ik\alpha}  = \]
				\[ \frac{1}{2\pi} \left\{ \int_{-\pi}^{\pi} \underbrace{1}_{k=0}  + \sum_{k<0} + \sum_{k>0} \right\} = 1 + \frac{1}{2\pi} \left\{ \int \sum_{k<0} + \sum_{k>0} \right\} \]

				Entonces agrupamos términos con potencias $k$ con los de potencia $-k$ y las sumas nos dan cosenos. Integrando obtenemos senos de $-\pi$ y $\pi$ (por los límites de la integral), y por tanto:
				\[\int_{-\pi}^{\pi} P(r, \alpha) d\alpha = 1\]
			\end{proof}


			\item \[
			\lim_{r \to 1^-} P(r, \alpha) = \begin{cases}
				\infty & \alpha = 0 \\
				0 & \alpha \neq 0
			\end{cases}
			\]

			\[
				\frac{1}{2\pi} \cdot \frac{1-r^2}{1+r^2-2r\cos \alpha} \rightarrow \begin{cases}
				\alpha = 0 & \frac{1}{2\pi} \frac{(1-r)(1+r)}{(1-r)^2} \ \rightarrow \infty \\
				\alpha \neq 0  & \frac{1}{2\pi} \frac{0}{\underbrace{2-2\cos \alpha}_{\neq 0}} \rightarrow 0
				\end{cases}
			\]

			Este es el comportamiento típico de una \concept{Delta de Dirac}.

			\begin{center}
			\begin{tikzpicture}

			\draw[-] (-3,0) -- (3,0) node [below] {$\theta$};
			\draw[-] (0,-0.5) -- (0,2);

			\draw[-] (-1.5,0.1) -- (-1.5,-0.1) node[below] {$-\pi$};
			\draw[-] (1.5,0.1) -- (1.5,-0.1) node[below] {$\pi$};

			\draw[blue, thick] plot[smooth] coordinates {(-1.5,0.2) (-0.5,0.5) (0,1.7) (0.5,0.5) (1.5,0.2)};

			\draw[green, thick] plot[smooth] coordinates {(-1.5,0.6) (-0.7,0.8) (0,1.5) (0.7,0.8) (1.5,0.6)};

			\node[right] at (1.5,0.6) {$r$ pequeño};
			\node[right] at (1.5,0.2) {$r$ próximo a 1};

			\end{tikzpicture}
			\end{center}

			\noindent Puesto que la función es periódica, da lo mismo integrar entre $-\pi$ y $\pi$ que cualquier otro intervalo de tamaño $2\pi$. Todas serán 1.

		\end{itemize}
		\end{prop}

		\begin{theorem}[Teorema\IS de Poisson] \label{thm:ConvUniformePoisson}
			$ $ % hack para forzar el salto de línea

			\noindent Supongamos que $g$ es una función continua y $2\pi$-periódica. Entonces:

			\[ \int^\pi_{-\pi} g(s)P(r, \theta-s) \dif s \convs[][r][1^-] g(\theta) \text{ uniformemente en } [-\pi,\pi] \]

			IMPORTANTE: Este es el primer teorema que vemos en el que se nos indica el límite a parte de darnos convergencia uniforme. Usaremos este teorema como base para ampliar los resultados anteriores.
		\end{theorem}

			\begin{proof}
				Para hacer la prueba vamos a estimar la diferencia entre ambas funciones:
				\[0 \leq \left| \int_{-\pi}^{\pi} g(s) P(r, \theta-s) \dif s  - g(\theta))  \right| \]

				Tenemos que ${\displaystyle \int_{-\pi}^{\pi}} P(r, \theta-s) \dif s = 1$, luego podemos meterlo dentro multiplicando a $g(θ)$ sin que nada cambie:
				\begin{align*}
				0	&≤ \left| \int_{-\pi}^{\pi} g(s) P(r, \theta-s) \dif s - g(\theta) \int_{-\pi}^{\pi} P(r, \theta-s) \dif s  \right| = \\
					&= \left| \int_{-\pi}^\pi (g(s) - g(\theta)) P(r, \theta - s) \dif s \right|
					≤ \int^{\pi}_{-\pi} | g(s) - g(\theta) | P(r, \theta- s)
				\end{align*}

				Hemos podido hacer este paso ya que $P>0$, pero veremos en unos días que no se va a cumplir en otra prueba y tendremos problemas.

				Vamos a descomponer la integral en dos trozos, en uno nos ayudaremos de la continuidad de $g$ y en el otro de la continuidad del núcleo.

				\[ = \underbrace{\int_{|\theta-s|< \delta}}_{|g(s) - g(\theta)| \text{ pequeño y }P\text{ grande}} + \underbrace{\int_{|\theta-s| > \delta}}_{|g(s) - g(\theta)| \text{ acotado y }P\text{ pequeño}} \]

				\proofpart{Primera integral, en $\abs{θ - s} < δ$}

				Para la primera integral, sabemos que $g$ es continua en $[-\pi,\pi]$, intervalo cerrado y acotado. Como $[-\pi,\pi]$ es compacto, entonces $g$ es {\bf uniformemente continua}\index{Función!uniformemente contínua}\footnote{Igual que la continuidad normal con la definición $ε-δ$, salvo porque el $δ$ no depende del punto que consideremos y nos vale para todo el intervalo. En este caso, como $g$ es continua en un compacto, para cada ε podemos tomar el δ máximo para cada punto, que existe y está bien definido.}.

				Dado $\epsilon > 0$, $\exists \delta > 0$ tal que si $|\theta-s| < \delta$, entonces $|g(s) - g(\theta)| < \epsilon, \forall \theta ∈ [-π, π]$. Así, podemos estimar la integral fácilmente:
				\[
					\int\limits_{|\theta - s| < \delta} \abs{g(s) - g(\theta)} P(r, \theta - s) \dif s < \epsilon \int\limits_{|\theta - s| < \delta} P < \epsilon \int_{-\pi}^{\pi} P = ε
				\]

				Luego tiende a 0, pero hay que tener cuidado porque ya hemos dejado fijado $\delta$.

				\proofpart{Segunda integral, en $\abs{θ - s} > δ$}

				\noindent Para el delta elegido en la integral anterior, podemos dividir la integral en dos partes:
				\begin{multline*}
				\int\limits_{|\theta - s | > \delta} \abs{g(s) - g(\theta)} P(r, \theta-s) \dif s = \\
				= \int_{\theta+\delta}^\pi \underbrace{|g(s) - g(\theta)| P(r, \theta-s) \dif s}_{(A)} +  \int_{-\pi}^{\theta-\delta} \underbrace{|g(s) - g(\theta)| P(r, \theta-s) \dif s}_{(B)}
				\end{multline*}

				Y tenemos:
				\[ (A) \leq P(r, \theta-(\theta+\delta)) = P(r, -\delta) = P(r, \delta) \leq  \]
				\[\leq P(r, \delta) \int^{\pi}_{\theta+\delta} |g(s) - g(\theta) ds \leq C P(r, \underbrace{\delta}_{> 0}) \convs[][r][1^-] 0  \]

				\[g \text{ continua } \Rightarrow |g| < M \]

				\[ |g(s)  - g(\theta)| \leq |g(s)| + | g(\theta) | \leq 2M \]


				Hemos probado:

				Dado $\epsilon > 0$, $\exists r_0$ tal que si $r \in (r_0,1)$:

				\[ \left| \int_{-\pi}^\pi g(s) P(r, \theta-s) ds - g(\theta)  \right| < \epsilon \quad \forall \theta \]

			\end{proof}

		\noindent La primera aplicación que nos da este teorema es muy sencilla y algo esperable:
		\begin{prop}
		Dadas dos funciones $f,g$ continuas con los mismos coeficientes de Fourier, entonces son la misma función\footnote{Misma función \textit{en $L^2$}, que es un detalle sutil. En $L^2$, dos funciones son equivalentes si y sólo si son iguales en casi todo punto. Es decir, que $f$ y $g$ podrían ser distintas, aunque sólo en un conjunto de medida cero.}.
		\end{prop}
		\begin{proof}
			\[
				\left. \begin{array}{l}
				f \\
				g
				\end{array} \right\} \rightarrow \alpha_k \rightarrow \sum_k \alpha_k\ r^{|k|} e^{ik\theta} \left\{ \begin{array}{l}
					{\displaystyle \int^{\pi}_{-\pi}} f(s) P(r, \theta - s)ds \convs[][r][1^-] f \\
					{\displaystyle \int^{\pi}_{-\pi}} g(s) P(r, \theta - s)ds \convs[][r][1^-] g
				\end{array} \right.
			\]

			Por unicidad del límite $\Rightarrow f \equiv g$
		\end{proof}

		% clase 2016/02/29

		Si tenemos un problema en una circunferencia de radio $a$, hacemos un reescalado:

		\[ \gor{W}(\rho, \theta) = W(\frac{\rho}{a},\theta) \quad \rho \in [0,a) \]

		Usando la regla de la cadena llegamos a:

		\[\gor{W}_{\rho \rho} + \frac{1}{\rho} \gor{W}_\rho + \frac{1}{\rho^2} \gor{W}_{\theta \theta} = 0, \quad \rho \in [0,a)  \]
		\[\gor{W}(\rho, \theta) = W(\frac{\rho}{a},\theta) = \int^{\pi}_{-\pi} g(s) \cdot {P(\frac{\rho}{a},\theta-s)} ds \]
		Donde $P(\frac{\rho}{a},\theta-s)$ es el \concept[Núcleo\IS de Poisson para la bola de radio $a$]{núcleo de Poisson para la bola de radio $a$}.

		Ya como consecuencias algo más interesantes del teorema de Poisson \ref{thm:ConvUniformePoisson} tenemos las siguientes proposiciones.

		\begin{prop}[Propiedad\IS de la media]

		 \[u(x,y)|_{(0,0)}  = W(0,\theta) = \int_{-\pi}^\pi g(s) P(0, \theta - s) ds = \frac{1}{2\pi} \int_{-\pi}^\pi g(s) ds \]
		% \[ u_{xx} + u_{yy} = 0\]
		\end{prop}

		Las funciones que verifican la condición de Laplace (las funciones armónicas) son muy rígidas y verifican que son el promedio de todos los puntos de la circunferencia que les rodea. Son las funciones que aparecen en los equilibrios, por ejemplo, en la función de ondas.

		Lo veremos más adelante, pero se puede imaginar una membrana, que cuando se queda parada, en una posición de equilibrio, los puntos interiores dependen de la forma del bastidor que sujeta la membrana.

		\begin{prop}[Propiedad\IS del máximo]
		$ $ % hack

		Si $m \leq g(\theta) \leq M, \forall \theta \in [-\pi,\pi]$, entonces $m \leq u(x,y) \leq M, \forall (x,y)$

		\[u(x,y) \rightarrow W(r,\theta) = \int_{-\pi}^{\pi} \underbrace{g(s)}_{\in [m,M]} \underbrace{P(r,\theta-s)}_{> 0} ds \ \begin{cases}
			\leq {\displaystyle \int_{-\pi}^{\pi}} M \cdot P(r,\theta-s)\  ds = M \\
			\geq {\displaystyle \int_{-\pi}^{\pi}} m \cdot P(r,\theta-s)\  ds = m

		\end{cases} \]

		\noindent Una función armónica alcanza mínimo y máximo en el borde, no puede alcanzarlo en el interior.
		\end{prop}

		\begin{prop}
			Sean $f$,$g$ funciones contínuas, $2\pi$-periódicas:
			\[ \int_{-\pi}^{\pi} f(s) e^{-iks}ds = \int_{-\pi}^{\pi} g(s) e^{-iks}ds,  \forall k \implies f \equiv g \]
		\end{prop}


		\begin{theorem}[Teorema\IS de aproximación de Weierstrass (I)] \label{thm:AproxWeierstrass1}
		$ $ % hack

		Sea $f$ contínua y $2\pi$-periódica. Entonces existe un polinomio trigonométrico:

		\[T_n (\theta) = \sum_{k = -n}^n c_k e^{ik\theta} \]

		tal que $\forall \epsilon > 0$ existe $n_0$ tal que si $n > n_0$: $|T_n (\theta) - f(\theta)| < \epsilon, \ \forall \theta \in [-\pi,\pi]$.
		\end{theorem}

		\begin{proof}
			Dada $f$, sabemos calcular sus coeficientes de Fourier mediante la fórmula
			\[ \alpha_k = \int_{-\pi}^\pi f(s) e^{-iks} ds \]

			Además sabemos que
			\[ \sum_{k=-\infty}^{\infty} \alpha_k\  r^{|k|} e^{ik\theta} \convs[][r][1^-] f(\theta), \text{ uniformemente por el Th de Poisson.} \]

			Entonces dado $\epsilon > 0$, $\exists \delta > 0$ tal que si $1-\delta < r < 1$ tenemos que:

			\( \left| \sum_{k=-\infty}^{\infty} \alpha_k\  r^{|k|} e^{ik\theta} - f(\theta) \right| < \epsilon, \ \forall \theta \in [-\pi,\pi]  \label{eq:thm-approx-Weierstrass-a} \)

			Fijamos $r_{*} \in (1-\delta, 1)$

			\( r_* < 1 \Rightarrow \sum_{k=-\infty}^\infty \alpha_k\  r_*^{|k|} e^{ik\theta} < \infty \eqreason[\Longrightarrow]{Dado $\epsilon > 0, \exists n_0$ tal que si $n \geq n_0$} \sum_{|k| > n} \alpha_k\  r_*^{|k|} e^{ik\theta} < \epsilon \label{eq:thm-approx-Weierstrass-b} \)

			Utilizando la \fref{eq:thm-approx-Weierstrass-a} y la \fref{eq:thm-approx-Weierstrass-b}:
			\[ \implies \left| \sum_{|k| < n} \alpha_k\  r_*^{|k|} e^{ik\theta} - f(\theta) \right| < 2 \epsilon, \forall \theta \in [-\pi,\pi] \]
			\obs La suma es finita y de polinomios trigonométricos.

			Por lo tanto cualquier función contínua se puede aproximar uniformemente por una suma finita de $\alpha_k\ r_*^{|k| e^{ik\theta}}$.

		\end{proof}

		\begin{theorem}[Teorema\IS de aproximación de Weierstrass (II)] \label{thm:AproxWeierstrass2}
		$ $ %hack

		\noindent Dada una función $f$ $2\pi$-periódica y continua. Para todo $\epsilon > 0$ existe un {\bf polinomio}:
		$$p(x)= a_0 + a_1 x + … + a_n x^n$$ tal que $|f(x)-p(x)| < \epsilon, \ \forall x \in [-\pi,\pi]$
		\end{theorem}

		\begin{proof}
		$ $ %hack

		\noindent Usar la misma prueba que con el primer teorema de aproximación de Weierstrass
		\end{proof}

		Con esto llegamos a la idea de que si la función $f$ fuera analítica tendríamos que se aproxima uniformemente en un intervalo por su polinomio de Taylor (como los senos y los cosenos).

		Si aproximamos una función con un polinomio trigonométrico, entonces lo tenemos aproximado por muchos términos con senos y cosenos, lo cual son funciones analíticas, que permiten aproximar cada uno de ellos a su polinomio de Taylor del orden que se quiera. Si acotamos cada término con ($\epsilon / $ número de términos) entonces el error total será menor a $\epsilon$.

		Tenemos el problema de que no nos dice cómo calcular ese $p(x)$, la prueba no es constructiva. Sin embargo, el teorema de funciones analíticas si te da esa solución, pero solo vale para funciones iguales a su polinomio de Taylor.

		\clearpage % FIXME: puede estropear la estructura del documento
		\subsection{Identificación del límite con convergencia $L^2$}
		\textbf{Aplicación} al estudio de la convergencia en $L^2$.

		\begin{theorem} \label{thm:ConvergenciaL2}
			Partimos de $\{\Phi_k\}$, con $\Phi_k(s) = \frac{e^{iks}}{\sqrt{2\pi}}$, un sistema ortonormal en $L^2([-\pi,\pi])$ y $f \in L^2$, con los coeficientes de Fourier
			\[\alpha_k = \pesc{f, \Phi_{k}} = \int_{-\pi}^\pi f(s) \overline{\Phi_k(s)} \dif s  \] y las sumas parciales
			\[  S_n f = \sum_{|k| < n} \alpha_k · \Phi_k = \sum_{|k| < n}\pesc{f, \Phi_k} \Phi_k \]

			Entonces, para todo $ε > 0$ podemos encontrar $T_n$ polinomio trigonométrico de grado $n$, tal que
			\[ \| S_n f - f\|_{L^2} \leq \| T_n - f \|_{L^2} < \epsilon \]

			La igualdad se dará sólo si $S_n \equiv T_n$.

		\end{theorem}

			\begin{proof}
				\[ T_n = \sum_{|k| < n} C_k \Phi_k(x) \]
				\begin{align*}
					\| T_n - f \|_{2}^2 &=  \pesc{T_n -f, T_n - f} = \pesc{\sum_{|k| < n} C_k \Phi_k(x) - f, \sum_{|j| < n} C_j \Phi_j(x) - f} =\\
					&= \pesc{\sum_{|k| < n} C_k \Phi_k(x), \sum_{|j| < n} C_j \Phi_j(x)} - \pesc{\sum_{|k| < n} C_k \Phi_k(x), f} - \pesc{f, \sum_{|j| < n} C_j \Phi_j(x)} + \pesc{f,f} \eqreason{$\Phi_k$ sistema ortonormal}\\
					&= \sum_{|k| < n} C_k^2 - \sum_{|k| < n} C_k \pesc{\Phi_k(x),f} - \sum_{|j| < n} \gor{C_j} \ \gor{\pesc{\Phi_j(x),f}} + \|f\|_2^2
				\end{align*}

				\begin{align*}
				\| S_n f - f \|_2^2 = … &= \pesc{\sum_{|k| < n} \pesc{f,\Phi_k(x)} \Phi_k -f,  \sum_{|j| < n} \pesc{f,\Phi_j(x)} \Phi_j -f} =\\
				= … &= \|f\|_2^2 - \sum_{|k| < n} |\pesc{f,\Phi_k(x)}|^2
				\end{align*}
				Entonces:
				\[ \| S_n f - f \|_2^2 - \|T_n -f\|_2^2 = \dots = - \left\{ \sum_{|k| < n} \underbrace{( \pesc{f,\Phi_k} - C_k) \overline{(\pesc{f,\Phi_k} - C_k)}}_{|\pesc{f, \Phi_k} - C_k|^2 } \right\} \eqreason[<]{por el -} 0 \]


			\end{proof}

		\newpage % FIXME: puede romper el documento
		\textbf{Consecuencias:}
		\begin{itemize}

			\item  \[ f \in L^2 \implies S_n f \eqexpl[\longrightarrow]{$L^2$} f \]

			Lo cual es consecuencia del resultado anterior junto con el primer teorema de aproximación de Weierstrass.

			\item \[ \| S_n f - f \|_{2}^2 = \| f \|^2_2 - \sum_{|k|<n} | \pesc{f, \Phi_k} |^2  \]

			Pero como $\| S_n f - f \|_{2}^2 \convs[L^2] 0$, entonces obtenemos la \concept{Identidad\IS de Parseval}:

			\( \sum_{k= -\infty}^\infty  |\pesc{f, \Phi_k}|^2 = \|f\|^2_2 \label{eq:identidad-parseval} \)


		\end{itemize}

		%clase 2016/03/01


		Aunque hemos hecho trampa, porque hemos aplicado un resultado válido solo para funciones contínuas a funciones que no sabíamos si lo eran. El siguiente teorema nos ayudará con ello:

		\begin{theorem} \label{thm:ConvergenciaL2Limite}

			\[ f \in L^2 \implies S_n f \eqexpl[\longrightarrow]{$L^2$} f \]

		\end{theorem}

		\begin{proof}

			\textbf{Caso 1:} Supongamos $f$ contínua.

			Por Weierstrass tenemos que dado $\epsilon > 0$, existe un polinomio $T_n$ tal que $|T_n(s) - f(s)| < \epsilon, \forall s \in [-\pi,\pi]$

			\(
				\int^{\pi}_{-\pi} | S_n f - f |^2 ds \leq \int^\pi_{-\pi} | T_n - f |^2 ds < 2 \pi \epsilon^2 \label{eq:resultadoContinuidadWeierstrass}
			\)

			\textbf{Caso 2:} $f \in L^2 ([-\pi,\pi])$

			Recordemos las dos definiciones equivalentes de $L^2$:
			\begin{itemize}

				\item $L^2 \equiv$ \{ funciones medibles tales que $\displaystyle \int^\pi_{-\pi} |f|^2 dx < \infty $ \}

				\item $L^2 \equiv$ cierre del conjunto de las funciones contínuas con la topología inducida por $\|\cdot\|_2$, que se relaciona con la \fref{eq:resultadoContinuidadWeierstrass}

			\end{itemize}

			Data $f\in L^2, \exists \set{f_n}$ de funciones contínuas tales que $\|f_n - f\|_2 \convs 0$

			Sea $f_\epsilon$ contínua con $\|f-f_\epsilon\|_2 < \epsilon$:

			Entonces,
			\begin{align*}
			\| S_N f - f \|_2 &= \| S_N f \pm S_N f_\epsilon \pm f_\epsilon - f \|_2 \leq \\
			& \leq \| S_N f - S_N f_\epsilon \|_2 + \| S_N f_\epsilon - f_\epsilon \|_2 + \underbrace{\| f_\epsilon - f \|_2}_{< \epsilon}
			\end{align*}

			Tenemos que $\| S_N f_\epsilon - f_\epsilon \|_2 \convs 0$ por el caso 1; y

			\[ \| S_N f - S_N f_\epsilon \|_2 \eqreason{sumas finitas} \| S_N (f - f_\epsilon) \|_2 \eqreason[\leq]{Bessel} \| f - f_\epsilon \|_2 < \epsilon \]

			Luego tenemos que $\| S_N f - f \|_2 < 2\epsilon$.

		\end{proof}

		\begin{corol}
		\begin{gather*}
			\begin{rcases}
				\| S_n f - f \|_2^2 = \|f\|_2^2 - \sum_{-n}^{n} |\pesc{f, \Phi_k}|^2 \\
				\|S_n f - f \|_2 \convs 0
			\end{rcases}
			\implies  \sum_{-\infty}^{\infty} | \pesc{f, \Phi}|^2 = \|f\|_2^2 \quad \text{\bf Identidad de Parseval} \index{Identidad!de Parseval}
		\end{gather*}

		con $\set{\Phi_k}$ base ortonormal. Esta fórmula nos permite calcular sumas infinitas.

		\end{corol}


	\subsection{Resultados probados}

		\begin{itemize}

			\item $f \in C^2, 2\pi$-periódica $\implies S_n f \rightarrow f$ uniformemente

			\item $f\in L^2 \implies S_n f \eqexpl[\rightarrow]{$L^2$} f$

			\item Desigualdad de Bessel (la \fref{eq:desigualdad_bessel}), Identidad de Parseval (la \fref{eq:identidad-parseval}).

			\item Convergencia uniforme de la ``serie modificada'' $\sum\limits_k \alpha_k \ r^{|k|} e^{ikx}$ (Poisson)

		\end{itemize}


	\subsection{Convergencia puntual: Teorema de Dirichlet}
	Comenzaremos esta sección desarrollando la herramienta con la que demostraremos la convergencia puntual de la serie de Fourier: el núcleo de Dirichlet.

	\subsubsection{Núcleo de Dirichlet}
	\begin{prop}[Núcleo\IS de Dirichlet]
		El núcleo de Dirichlet tiene la siguiente expresión
		\[ D_n(t) = \frac{1}{2 L} \frac{\sin((n + \frac{1}{2}) \frac{\pi}{L} t)}{\sin( \frac{1}{2} \frac{\pi}{L} t)} \]
		Pero como nos vamos a remitir al intervalo $[-\pi, \pi]$, a partir de ahora consideraremos
		\( D_n(t) = \frac{1}{2 \pi} \frac{\sin((n + \frac{1}{2}) t)}{\sin(\frac{t}{2})} \label{eq:nucleoDirichlet} \)
	\end{prop}
	A continuación, vamos a deducir la expresión del núcleo:
	\begin{proof}
		Consideramos el sistema ortonormal en $[-\pi,\pi]$:
		\[ \set{\Phi_k} = \set{\frac{1}{\sqrt{2\pi}} e^{ikx}}_{k \in \ent} \]

		Sea f ``regular'', ie, todo lo regular que necesitemos:
		\[S_nf = \sum_{k=-n}^{n} \pesc{f,\Phi_k} \Phi_k(x) = \sum_{-n}^n \left( \int_{-\pi}^\pi f(s) \frac{1}{\sqrt{2\pi}} e^{-iks} ds \right)  \frac{1}{\sqrt{2\pi}} e^{ikx} \]

		\[ = \sum_{-n}^{n}  \int_{-\pi}^\pi f(s) \frac{1}{2\pi} e^{-ik(s-x)} ds \eqreason{suma finita} \int_{-\pi}^{\pi} f(s) \frac{1}{2\pi} \sum_{-n}^n e^{-ik(s-x)} ds \]

		Continuamos con solo el sumatorio de la ecuación anterior: es una progresión geométrica, así que la sabemos calcular (salvo en el límite):
		\[ \frac{1}{2\pi} \sum_{-n}^n e^{-ik(s-x)} = \frac{1}{2\pi} \sum_{-n}^n e^{ik(x-s)} = \frac{1}{2\pi} \sum_{-n}^n \left[\underbrace{e^{i(x-s)} }_{\equiv \rho}\right]^k \]

		Definimos $\rho = e^{i(x-s)}$ y aplicamos la fórmula de la progresión geométrica:
		\[ \frac{1}{2\pi} \sum_{-n}^n \rho^k = \frac{1}{2\pi} \frac{\rho^{-n} - \rho^{n+1}}{1 -\rho} \]

		Sea $t = x- s$. Sustituimos de vuelta y operamos:
		\begin{gather*}
		\frac{1}{2\pi}  \sum_{-n}^{n} (e^{it})^k = \frac{1}{2\pi} \frac{e^{-nit}-e^{(n+1)it}}{1-e^{it}} = \frac{1}{2\pi} \frac{e^{-it/2}}{e^{-it/2}} \cdot \frac{ e^{-nit}-e^{(n+1)it}}{1-e^{it}} =\\
		= \frac{1}{2\pi} \frac{e^{-(n + 1/2)it}-e^{(n+1/2)it} }{e^{-it/2}-e^{it/2}} \cdot \frac{-1}{-1} = \frac{1}{2\pi} \frac{e^{(n + 1/2)it} - e^{-(n+1/2)it} }{e^{it/2}-e^{-it/2}} \eqreason{fórmula de Euler} \dots
		\end{gather*}
		\[ … = \frac{1}{2\pi} \frac{\sin(n+ \frac{1}{2})t}{\sin(\frac{t}{2})}  \equiv D_n(t) \]
	\end{proof}

	Pero nos falta por estudiar cuál es la relación entre las propiedades del núcleo de Dirichlet cuando n tiende a infinito y el límite de $S_N f$.


	\begin{figure}[hbtp]
	\centering
	\inputtikz{PoissonDirichlet}
	\caption{Núcleo de Poisson $P(s,t)$ (verde)  y de Dirichlet $D_n(t)$ (naranja). Gráfica más oscura implica valores mayores de $s$ y $n$ respectivamente.}
	\label{fig:PoissonDirichlet}
	\end{figure}

	En la \fref{fig:PoissonDirichlet} comparamos los núcleos de Poisson y Dirichlet. Como podemos ver, Dirichlet plantea problemas de convergencia.

	El argumento que debemos utilizar debe basarse en las oscilaciones, ya que no tenemos nada más que nos lo permita. Tendremos una integral que queremos hacer tender a cero por culpa de las oscilaciones de dentro. El teorema que nos da este tipo de argumentos es el de Riemann-Lebesgue. Veamos:

	Fijando x, queremos estimar:

	\( |S_n f(x) - f(x) | = \left| \int_{-\pi}^\pi f(s) D_n (x-s) ds - f(x) \right| \label{eq:dirichletFijandoX} \)

	\subsubsection{Propiedades del núcleo de Dirichlet}
	\begin{prop}[Propiedades\IS del núcleo de Dirichlet]
	$ $ % hack

	\begin{itemize}
		\item $D_n(-t) = D_n(t)$
		\item $\displaystyle \int_{-\pi}^\pi  \frac{1}{2\pi} \sum_{-n}^{n} e^{ikt} dt = \int_{-\pi}^{\pi} D_n(t) dt = 1, \forall n $
	\end{itemize}
	\end{prop}


	Entonces, para simplificar la integral hacemos el cambio $x-s = t$:
	\obs al ajustar los límites: $\pi = x-t \implies t = x-\pi$, ¡los signos se dan la vuelta!, pero como $ds = - dt$, quedan bien.
	\[  \int_{x+\pi}^{x -\pi} f(x-t) D_n(t) (-dt) \eqreason{$2\pi$-periódica} \int^{\pi}_{-\pi} f(x-t) D_n(t) dt \]
	Luego
	\[ \eqref{eq:dirichletFijandoX} = \left| \int_{-\pi}^\pi (f(x-t)-f(x)) \frac{1}{2\pi} \frac{\sin((n+\frac{1}{2})t)}{\sin(\frac{t}{2})} dt \right| \]

	% clase 7/3/2016

	\begin{theorem}[Teorema\IS de convergencia puntual de Dirichlet (1)] \label{thm:Dirichlet1}
		$ $ % hack

		Sea $f$ medible y acotada derivable en $x$. Entonces $$S_n f(x) \convs f(x)$$
	\end{theorem}

	\begin{proof}
		\begin{gather*}
			0 \leq |S_nf(x) - f(x) | \eqreasonup{la integral del núcleo es 1} \left| \int_{-\pi}^{\pi} f(x-t) D_n(t) dt - f(x) \int_{-\pi}^\pi D_n(t) dt \right| \eqreason{agrupamos integrales} \\
			= \left| \int_{-\pi}^\pi (f(x-t)-f(x)) D_n(t) dt) \right| \eqreason{sustituimos $D_n(t)$} \frac{1}{2\pi} \left| \int_{-\pi}^\pi \frac{f(x-t)-f(x)}{\sin (t/2)} \cdot \sin((\pi+1/2)t) \right| \eqreason{$\pi = (\sqrt{\pi})^2$} \\
			= \frac{1}{2} \left| \int_{-\pi}^\pi \frac{f(x-t)-f(x)}{\sqrt{\pi}\sin (t/2)} \cdot \frac{\sin((n+1/2)t)}{\sqrt{\pi}} \right|
		\end{gather*}


		Utilizamos dos lemas para completar la demostración:

		\begin{lemma}
			$\set{\frac{1}{\sqrt{\pi}} \sin((n + \frac{1}{2}) t)} $ es una familia ortonormal en $[-\pi,\pi]$
		\end{lemma}
		La demostración se deja como ejercicio al lector.

		\begin{lemma}
			En las hipótesis del teorema:
			\[ \frac{f(x-t)-f(x)}{\sin(t/2)} \eqreason[\in]{variable $t$, $x$ fijo} L^2 ([-\pi,\pi]) \]
		\end{lemma}

			\begin{proof}
				Sea
				\[ g(t) = \frac{f(x-t)-f(x)}{\sin(t/2)} = \underbrace{\frac{f(x-t)-f(x)}{-t}}_{\convs[ ][t][0] f'(t)} \underbrace{\frac{-t}{\sin(t/2)}}_{\convs[ ][t][0] -2} \]
				Entonces existe un $\delta > 0$ tal que $\| g(t) \| < C$, si $|t| < \delta$.

				Observemos que el seno entre $-\pi$ y $\pi$ es impar, y que si quitamos una bola centrada en el 0 de radio $\delta$, tenemos que $\| \frac{1}{\sin(t/2)} \| < k$, si $\| t \| < \delta$.

				Como f es derivable y tenemos un intervalo acotado, tenemos que f está acotada, luego $\| f(x-t) - f(x) \| \leq 2 M$.

				Por tanto, $\| g(t) \| < \gor{C}$, $\forall t \in [-\pi,\pi]$.

				Finalmente,
				\[ \int\limits_{-\pi}^{\pi} \| g(t) \|^2 dt < 2M \gor{C} \implies g \in L^2 \]
			\end{proof}

		{\bf Conclusión:}

		Lema 1 + Lema 2 + Riemman-Lebesgue $\implies \left| S_n f(x) - f(x) \right| \longrightarrow 0$

	\end{proof}

	\newpage % FIXME: puede romper la estructura del documento
	\begin{theorem}[Teorema\IS de convergencia puntual de Dirichlet (2)] \label{thm:Dirichlet2}
		$ $ % hack

		Sea $f$ medible y acotada. Además solo vamos a permitir discontinuidades de salto, dientes de sierra, etc.. pero vamos a evitar funciones con derivadas infinitas en algún punto. Admitimos límites y derivadas laterales siempre que sean finitos:

			\inputtikz{FuncionesDirichlet}
			\captionof{figure}{En rojo las que no permitimos, en verde las que sí.}
			\label{fig:FuncionesDirichlet}

		\begin{gather*}
		\begin{rcases}
			f(x_0^+) = \lim\limits_{x \to x_0^+} f(x) \\
			f(x_0^-) = \lim\limits_{x \to x_0^-} f(x) \\
			f'(x_0^+) = \lim\limits_{h \to 0^+} \frac{f(x_0+h)-f(x_0^+)}{h} \\
			f'(x_0^-) = \lim\limits_{h \to 0^-} \frac{f(x_0+h)-f(x_0^-)}{h}
		\end{rcases} \text{ existen y son finitos}\\
		\text{Entonces: } \ \lim_{n \to \infty} S_nf(x) = \frac{1}{2} \{f(x^+)+f(x^-)\}
		\end{gather*}

	\end{theorem}

	\begin{proof}
		\begin{gather*}
			0 \leq | S_n f(x) - \frac{1}{2} \{f(x^+) + f(x^-)\} | = \left| \int_{-\pi}^\pi f(x-t) D_n (t) dt - \underbrace{\frac{1}{2}}_{a} f(x^+)- \underbrace{\frac{1}{2}}_{b} f(x^-) \right|
		\end{gather*}
		Tenemos que:
		\begin{align*}
			\int_{-\pi}^\pi f(x-t) D_n (t) dt &= \int_{-\pi}^{0} f(x-t) D_n(t)dt + \int_{0}^\pi f(x-t) D_n(t) dt\\
			\textbf{a} = \frac{1}{2} &= \int_0^\pi D_n (t) dt \\
			\textbf{b} = \frac{1}{2} &= \int_{-\pi}^0 D_n (t) dt \\
		\end{align*}
		Luego sustituyendo y agrupando según los límites de integración obtenemos:
		\[ \left| \underbrace{\int_0^\pi  \{ f(x-t) - f(x^+) \} D_n(t) dt }_{\text{I}} + \underbrace{\int_{-\pi}^{0}  \{ f(x-t) - f(x^-) \} D_n(t) dt }_{\text{II}} \right| \]

		\[\leq | \text{I} | +  |\text{II}| \]

		Si nos fijamos en I:
		\[ I = \frac{1}{2\sqrt{\pi}} \int_{0}^\pi \underbrace{\frac{f(x-t)-f(x)}{\sin(t/2)}}_{=g(t)} \cdot \frac{\sin((n+1/2)t)}{\sqrt{\pi}} \ dt \]

		Ojo, $\frac{\sin((n+1/2)t)}{\sqrt{\pi}}$ es un sistema ortonormal en $[-\pi,\pi]$, pero no es siquiera ortogonal en $[0,\pi]$.

		Tenemos
		\[ \int_{0}^\pi \underbrace{g(t)}_{\in L^2} \underbrace{\sin((n+1/2)t)}_{\text{impar}} dt \]
		y lo que debemos hacer es extenderla a $[-\pi,\pi]$.

		Consideramos una función $\gor{g}$, que será la extensión IMPAR de $g$ en el intervalo $[-\pi,\pi]$. Entonces:
		\[ \int_{-\pi}^\pi  \gor{g}(t) \sin((n+1/2)t) \ dt \eqreasonup[\longrightarrow]{Riemann-Lebesgue} 0 \]
		Como es el producto de dos funciones impares, tenemos que su producto es par, luego
		\[ \int_{-\pi}^\pi  \gor{g}(t) \sin((n+1/2)t) \ dt = 2 \int_{0}^\pi g(t) \sin((n + \frac{1}{2})t)\ dt \convs 0 \]
		luego
		\[ \int_{0}^\pi g(t) \sin((n + \frac{1}{2})t)\ dt \convs 0 \]

		\noindent Para II se hace otra extensión igual pero extendiendo de ``izquierda a derecha''.

		\noindent Por tanto, las dos integrales se van a 0 y eso implica que $$0 \leq | S_n f(x) - \frac{1}{2} \{f(x^+) + f(x^-)\} | \leq 0$$
	\end{proof}

	\newpage % FIXME: puede romper la estructura del documento
	\subsubsection{Aplicación 1: cálculo de series}
	\begin{example}[Problema\IS de la suma de Basilea]$ $ % hack

		\noindent Dada $f(x) = x(\pi-x), x \in [0,\pi]$, hacemos su extensión par, como se puede ver en la \fref{fig:extension-par-03-07}:
		\[ f_1(x) =
		\begin{cases}
			f(x) & x \in [0, \pi]\\
			f(-x) & x \in [-\pi, 0]\\
		\end{cases} \]

		\begin{figure}[hbtp]
		\centering
		\begin{subfigure}[t]{.5\textwidth}
			\centering
			\inputtikz{Joroba-Ejemplo03-07}
			\caption{$f$ es la joroba de un dromedario.}
			\label{fig:Joroba-Ejemplo03-07}
		\end{subfigure}%
		\begin{subfigure}[t]{.5\textwidth}
			\centering
			\inputtikz{Camello-Ejemplo03-07}
			\caption{$f_1$ es parte del perfil de un camello.}
			\label{fig:Camello-Ejemplo03-07}
		\end{subfigure}
		\caption{Los colores son totalmente arbitrarios...}
		\label{fig:extension-par-03-07}
		\end{figure}


		Consideramos la base
		\[\set{ 1, \sin{kx}, \cos{kx} }_{k = ±1,±2,…} \]

		\obs No está normalizada, si quisiéramos usar Parseval, tendríamos que normalizar.

		El teorema de Dirichlet nos garantiza lo siguiente:
		\[
			f_1(x) = \frac{a_0}{2} + \sum_{k=1}^\infty a_k \cos(kx) + \sum_{k=1}^\infty b_k \sin (kx)
		\]
		Vamos a ver que $b_k = 0 \ \forall x$: ya que $f_1$ es par, tenemos que
		\[b_k = \frac{1}{\pi} \int_{-\pi}^{\pi} \underbrace{f_1(x) \sin (kx)}_{\text{impar}} \ \dif x = 0 \]
		Luego $a_0$ es:
		\begin{align*}
		a_0 &= \frac{1}{\pi} \int_{-\pi}^{\pi} \underbrace{f_1(x)}_{\text{par}} \dif x = \frac{2}{\pi} \int_0^\pi x(\pi-x) \dif x = \frac{2}{\pi} \left(\left. \frac{\pi}{2}x^2 - \frac{x^3}{3} \right|_{0}^{\pi} \right) = \frac{\pi^2}{3} \\
		a_k &= \frac{1}{\pi} \int_{-\pi}^{\pi} \underbrace{f_1(x) \cos (kx)}_{\text{par}} \dif x = \frac{2}{\pi} \int_{0}^{\pi} x(\pi - x) \cos (kx) \dif x = \\
		   &= \frac{2}{\pi} \left\{ \pi \int_{0}^{\pi} x \cos(kx) \dif x  - \int_{0}^{π} x^2 \cos(kx) \dif x \right\} \eqreasonup{por partes dos veces} \dots =
		\begin{cases}
			\frac{-4}{k^2} & k \text{ par} \\
			0 & k \text{ impar}
		\end{cases}
		\end{align*}

		Luego tenemos que $\frac{a_0}{2} = \frac{\pi^2}{6}$ y $a_{2n} = \frac{-4}{(2n)^2} = \frac{-1}{n^2}$

		\obs si hacemos una extensión {\bf par}, tendrá periodo $\pi$; y si hacemos una extensión {\bf impar} tendrá periodo $2\pi$.

		\noindent {\bf Conclusión:}
		\[ x(\pi - x) = \frac{\pi^2}{6} -\sum_{n=1}^{\infty} \frac{1}{n^2} \cos(2n x) \]
		En particular, si $x=0$:
		\[ 0 = \frac{π^2}{6}  - \sum_{n=1}^\infty \frac{1}{n^2} \implies  \sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6} \]

		Este resultado fue probado por Euler, y la suma de $\frac{1}{n^2}$ se la conoce como \concept{Problema de la suma de Basilea}.

		Lo más interesante es que a partir de ésta, Euler pudo calcular otras series, por ejemplo:
		\begin{gather*}
			\frac{\pi^2}{6} = \sum_n \frac{1}{n^2} = \sum_{n \text{ par}} \frac{1}{n^2} + \sum_{n \text{ impar}} \frac{1}{n^2} =\\
			= \sum_{j=1}^\infty \frac{1}{(2j)^2} + \sum_{j=1}^\infty \frac{1}{(2j-1)^2} = \frac{1}{4} \underbrace{\sum_{j=1}^\infty \frac{1}{j^2}}_{= \frac{\pi^2}{6}} \ + \ \sum_{j=1}^\infty \frac{1}{(2j-1)^2}
		\end{gather*}

		Despejando obtenemos que
		\[ \sum_{j=1}^\infty \frac{1}{(2j-1)^2} = \frac{\pi^2}{6} - \frac{1}{4} \frac{\pi^2}{6} = \frac{\pi^2}{8} = 1 + \frac{1}{3^2} + \frac{1}{5^2} + … \]
	\end{example}

		\begin{example}[2: extensión impar]$ $ % hack

		\begin{minipage}[t]{0.6\textwidth}
			\vspace{0pt}
			Veamos $f_2$ como la extensión impar de la $f$ anterior:

			\[ f_2 =
			\begin{cases}
				f(x) & x \in [0, \pi]\\
				-f(-x) & x \in [-\pi, 0]\\
			 \end{cases} \]
		\end{minipage}
		\begin{minipage}[t]{0.4\textwidth}
			\vspace{0pt}
			\inputtikz{EjemploExtensionImpar-03-07}
			\captionof{figure}{La extensión impar a $[-\pi,\pi]$ de $x(\pi-x)$}
			\label{fig:EjemploExtensionImpar-03-07}
		\end{minipage}

		Tenemos pues que la $f_2$ es:
		\[ f_2(x) = \underbrace{\frac{a_0}{2}}_{0} + \sum_k \underbrace{a_k \cos(kx)}_{0} + \sum_k b_k \sin(kx) =  \sum_k b_k \sin(kx) \]
		luego
		\[ x(\pi - x) = \frac{8}{\pi} \sum_{n=1}^{\infty} \frac{\sin((2n-1)x)}{(2n-1)^3} \]
		\[\text{Si } x = \frac{\pi}{2} \longrightarrow \frac{\pi^2}{4} = \frac{8}{\pi} \sum_{n=1}^{\infty} \frac{\sin((2n-1)\frac{\pi}{2})}{(2n-1)^3} = 1 - \frac{1}{3^3} + \frac{1}{5^3} - \frac{1}{7^3} + … \implies \frac{8}{\pi} \sum_{n=1}^{\infty} \frac{\sin((2n-1)\frac{\pi}{2})}{(2n-1)^3} = \frac{\pi^3}{32}\]

		\obs Acabamos de ver que el resultado {\bf depende de} si la extensión que hacemos es par o impar.
	\end{example}








% Clase 8/3/2016
	\subsubsection{Aplicación de la identidad de Parseval}
		Al igual que con el teorema de Dirichlet, podemos ver la aplicación de la identidad de Parçeval, que nos decía que \[ \norm{f}_{L^2}^2 = \sum_{k ∈ ℤ} \abs{\pesc{f, φ_k}}^2 \] con $\set{φ_k}$ nuestro sistema ortonormal.

		Consideramos de nuevo la extensión par de $f(x) = x(π-x)$. Podemos calcular y ver que \[ \pesc{\tilde{f}, φ_0} = \frac{1}{\sqrt{2π}} 2 \int_0^π x(π-x) \dif x = \sqrt{\frac{2}{π}} \left( \left. \frac{πx^2}{2} - \frac{x^3}{3}\right|_{x = 0}^π \right) =  \sqrt{\frac{2}{π}} \frac{π^3}{6} \]

		Para el resto de los coeficientes, vemos primero que los coeficientes de los senos van a ser $0$ por simetría. En el caso de los cosenos, vemos que \[ \pesc{\tilde{f}, \frac{\cos kx}{\sqrt{π}}} = \frac{2}{\sqrt{π}} \int_{0}^π x(π-x) \cos kx \dif x = \frac{2}{\sqrt{π}} \left(π\int_0^π x \cos kx \dif x - \int_0^π x^2 \cos kx \dif x \right) \]

		Resolvemos las dos integrales por separado: \[ \int_0^π x \cos kx \dif x = x \eval{\frac{\sin kx}{k}}_{x=0}^π - \int_0^π \frac{\sin kx}{k} \dif x = \eval{\frac{\cos kx}{k^2}}_{x=0}^π = \frac{(-1)^k - 1}{k^2} \] y la otra integral sale \[ \int_0^π x^2 \cos kx \dif x = \dotsb = \frac{2π}{k^2} (-1)^k \]

		Juntando ahora y haciendo más cuentas, queda que \[ \pesc{\tilde{f}, \frac{\cos kx}{\sqrt{π}}} = \frac{2 \sqrt{π}}{k^2}\left(-1 - (-1)^k\right) = \begin{cases} 0 & k \text{ impar} \\ \frac{-4\sqrt{π}}{k^2} & k \text{ par} \end{cases} \]

		Por otra parte, vemos que \[ \norm{\tilde{f}}^2_{L^2} = \dotsb =\frac{π^5}{15} \]

		Así, la identidad de Parçeval nos dice que \[ \frac{π^5}{15} = \left(\sqrt{\frac{2}{π}} \frac{π^3}{6}\right)^2 + \sum_{n=1}^∞ \left(\frac{2\sqrt{π}}{(2n)^2} - ( -2)\right)^2 = \frac{π^5}{18} + \sum_{n=1}^{∞} \frac{π}{n^4} \] y que simplificando \[ \sum_{n=1}^∞ \frac{1}{n^4} = \frac{π^4}{90} \]

		Incluso podríamos repetir el resultado que hacíamos antes y sacar todavía más resultados de teoría de números, separando la suma de los pares y los impares \[ \frac{π^4}{90} = \sum_{k ∈ ℕ} \frac{1}{(2k)^4} + \sum_{k ∈ ℕ} \frac{1}{(2k-1)^4} = \sum_{k∈ℕ} \frac{1}{16k^4} + \sum_{k ∈ ℕ} \frac{1}{(2k-1)^4} \] y entonces podemos despejar y ver que \[ \sum_{k ∈ ℕ} \frac{1}{(2k-1)^4} =  \frac{π^4}{90} \frac{15}{16} \]


\subsection{Convergencia puntual para funciones Hölder continuas}

Recuperando lo que teníamos antes, teníamos que la continuidad no nos bastaba para tener convergencia de la serie de Fourier. Du Bois-Raymond construía un ejemplo de una función continua cuya serie no convergía en un punto, y Kolmogorov iba un poco más allá con una función continua cuya serie divergía en todo punto.

Pediremos algo más que continuidad:

\begin{defn}[Función\IS continua Hölder]\label{def:FuncContinuaHolder} Se dice que una función $\appl{f}{X}{ℝ}$ es Hölder continua si existe un $α ∈ [0,1]$ y $K ∈ ℝ$ tal que $∀t,s ∈ X$ (por ejemplo, $X = [-π, π]$) se cumple que \[ \abs{f(t) -f(s)} < K \abs{t-s}^α\]
\end{defn}

\begin{theorem} \label{thm:ConvPuntualHolder} Sea $\appl{f}{X}{ℝ}$ una función Hölder continua. Entonces su serie de Fourier converge en todo punto a $f(x)$.
\end{theorem}

El punto de partida es el de siempre: tenemos que \[ \sum_{n = 0}^∞ a_n ≝ \lim_{N \to ∞} S_N\qquad\quad S_N = \sum_{n=0}^N a_n\]

Para tratar esta suma introducimos el concepto de la \concept{Sumabilidad\IS Cèsaro}, definiendo las series de la media de las sumas parciales \[ σ_N = \frac{S_0 + \dotsb +  S_N}{N+1}\]

Estas series tienen límite si lo tienen las sumas parciales:

\begin{theorem} Si $\lim S_N = L$, entonces $\lim σ_N = L$. El recíproco no siempre es cierto.
\end{theorem}

Un ejemplo es ver que la serie $(-1)^n$ sí es sumable según Cèsaro, aunque no en el sentido habitual. Lo peculiar de esta serie es que oscila, y esto nos hace pensar que también se podría aplicar a las series de Fourier, que igualmente oscilan con los senos y cosenos.

\begin{theorem}[Núcleo\IS de Féjer]
	\[ σ_Nf = \int_{-π}^π f(x) F_N(x-t) \dif t \] donde $F_N$ son los {\bf núcleos de Féjer}, que no son más que el promedio de los núcleos de Dirichlet \[ F_N(t) = \frac{1}{N+1} \sum_{k=0}^{N-1} D_k(t) = \frac{1}{N+1} \left(\frac{\sin\left(\frac{N+1}{2}\right) t}{\sin \frac{t}{2}}\right)^2\]
\end{theorem}

Lo interesante del núcleo de Féjer es que es positivo y que nos permitirá hacer cuentas igual que hacíamos con el núcleo de Poisson (la \fref{eq:NucleoPoisson}).

\subsubsection{Aplicación del Núcleo de Féjer a las EDPs}

	\textbf{Aplicación a la ecuación del calor}

	Recordemos la ecuación que teníamos del calor: \begin{align*}
	u_t - u_{xx} = 0 & \quad x ∈ (0,L),\, t > 0 \\
	u(0,t) = u(L,t) = 0 & \quad  t > 0 \\
	u(x,0) = f(x)
	\end{align*}

	Usando el método de separación de variables, llegábamos a que la solución era \[ u(x,t) \qeq \sum_{k ∈ ℤ} a_k\ e^{-\left(\frac{kπ}{L}\right)^2 t} \sin (\frac{kπ}{L} x) \] donde \[ f(x) \qeq \sum_{k ∈ ℤ} a_k\ \sin (\frac{kπ}{L} x) \]

	Teníamos tres interpretaciones: convergencia uniforme si tenemos $f ∈ C^2$ periódica, convergencia en sentido $L^2$ y convergencia puntual con el teorema de Dirichlet.

	Un teorema (que habría que poner un poquito mejor):

	\begin{theorem} \label{thm:DerivadaFourier}
		Sean $\set{f_k}$ una sucesión de funciones derivables.

		\noindent Si $\sum f_k = f$, con convergencia uniforme, y $\sum f_k' = g$, con convergencia uniforme; entonces $$f \text{ es derivable y } f' = g.$$
	\end{theorem}
	\begin{proof}
		La prueba sale en el libro de Spivak \cite{spivak08}, aplicando el Teorema del Valor Medio y observando que las $f_k$ son derivables por ser senos y cosenos.
	\end{proof}

	Con esto, podemos derivar la solución $u(x,t)$ y vemos que \( u_t(x,t) \qeq \sum_{k ∈ ℤ} a_k \left[-\left(\frac{kπ}{L}\right)^2\right]  e^{-\left(\frac{kπ}{L}\right)^2 t} \sin (\frac{kπ}{L} x) \label{eq:ut-aplic1-fejer} \)

	Aplicamos aquí el siguiente lema:

	\begin{lemma}
		Sean $α, β > 0$. Entonces si la siguiente suma converge: \[ \sum_{k ∈ ℤ} k^α e^{-k^2β} < ∞\] entonces converge uniformemente.
	\end{lemma}

	Como
	\[ \abs{a_k \left[-\left(\frac{kπ}{L}\right)^2\right]  e^{-\left(\frac{kπ}{L}\right)^2 t} \sin (\frac{kπ}{L} x) } \eqreason[\leq]{$\abs{\sin()} \leq 1$} C k^2  e^{-\left(\frac{kπ}{L}\right)^2 t} \] luego por el lema anterior tenemos que la suma en valor absoluto converge uniformemente, y por el criterio de Weierstrass la suma la \fref{eq:ut-aplic1-fejer} converge uniformemente.

	Entonces tenemos que efectivamente $u_t$ es exactamente la \fref{eq:ut-aplic1-fejer}. De manera análoga, podemos hacer un argumento similar para $u_{xx}$.





	% Clase 9/3/2016

		\textbf{Aplicación a la ecuación de ondas}
			\[  \begin{cases}
				u_{tt} - u_{xx} = 0, & x \in (0, L), t > 0\\
				u(0,t) = u(L,t) = 0, & t > 0 \\
				u(x,0) = f(x) \\
				u_t(x,0) = g(x) \equiv 0 & \text{ (simplificamos las cuentas)}
				\end{cases}
			\]

			Por separación de variables y como $g \equiv 0$:

			\[
			\begin{cases}
				u(x,t) \qeq \sum\limits_{k=1}^\infty a_k \cos(\frac{k\pi}{L}t) \sin(\frac{k\pi}{L}x) \\
				\text{ donde } f(x) \qeq \sum\limits_{k=1}^\infty a_k \sin(\frac{k\pi}{L}x)
			\end{cases}
			\]

			Realizamos la extensión impar de $f$ al intervalo $[-L,L]$ para que los coeficientes de los cosenos se anulen.

			Entonces podemos escribir la serie de $u$. Como veíamos, tenemos 3 interpretaciones de la igualdad, dependiendo del tipo de convergencia que estemos hablando.

			Toda la regularidad y convergencia que vamos a conseguir viene de los $a_k$, ya que no tenemos exponenciales, solo senos y cosenos que oscilan. Lo que quiere decir que la serie que vamos a obtener será ``buena'' si la serie dato es buena, y mala si no.

			El criterio que teníamos para regularidad era el decaimiento de la serie, como los senos y cosenos están acotados y su valor absoluto será como máximo uno, entonces sus factores (los $a_k$) son los que influyen en esa convergencia. Esto se puede extrapolar a la fórmula D'Alambert (la \fref{eq:DALEMBERT}).


			Miremos una extensión de la serie de los $a_k$:
			\[a_1 \cos\left(\frac{\pi}{L}t \right) \sin\left(\frac{\pi}{L}x \right) + a_2 \cos\left(\frac{2\pi}{L}t \right) \sin\left(\frac{2\pi}{L}x \right) + a_3 \cos\left(\frac{3\pi}{L}t \right) \sin\left(\frac{3\pi}{L}x \right) + …\]


			\begin{figure}[hbtp]
				\begin{minipage}[m]{.33\linewidth}
					\inputtikz{Ondas-PrimerArmonico}
				\end{minipage}
				\begin{minipage}[m]{.33\linewidth}
					\inputtikz{Ondas-SegundoArmonico}
				\end{minipage}
				\begin{minipage}[m]{.33\linewidth}
					\inputtikz{Ondas-n-esimoArmonico}
				\end{minipage}
				\caption{Algunos armónicos}
				\label{fig:ondas-armonicos}
			\end{figure}


			Por Riemann-Lebesgue (\ref{lem:RiemannLebesgue}) tenemos que $a_k \to 0$.
			\obs El $a_1$ se llama el \concept[Armónico fundamental]{armónico fundamental}.


	\subsection{Extensiones}

		\subsubsection{Transformada de Fourier}
			Estudiamos el espectro de la función. Lo usamos cuando no conocemos la longitud de la cuerda. Llamamos a la \concept[Transformada\IS de Fourier]{Transformada de Fourier} de $f(x)$:
					\[\int_{-\infty}^\infty f(x) e^{-i \xi x} dx = \gor{f} (\xi) \]

		\subsubsection{Transformada de Fourier discreta (DFT)}

			Supongamos que tenemos $f(x)$, $2\pi$-periódica.

			\[ f(x) = \sum_{k=-\infty}^\infty c_k\ e^{ikx}\]

			Realizamos una grabación digital de sonido realizando un muestreo. Es decir, el micrófono nos da el valor de la función para muchos tiempos discretos.

			\begin{figure}[hbtp]
				\begin{minipage}[t]{.5\linewidth}
					\inputtikz{DFT-Muestreo}
				\end{minipage}
				\begin{minipage}[t]{.5\linewidth}
					\inputtikz{DFT-Interpolacion}
				\end{minipage}
				\caption{ Hacemos un muestreo de (discretizamos) la señal e interpolamos. }
				\label{fig:DFT-Sampling}
			\end{figure}

			Nuestro propósito es resumir esa información para poder guardarla de forma comprimida. Intentaremos hacer una interpolación para reconstruir la función inicial $f$. Después calcularemos la serie de Fourier de ésta, y por último truncaremos las frecuencias no audibles para comprimir.

			Este proceso sería muy costoso así que en vez de hacer todos los pasos uno por uno vamos a intentar no tener que pasar por el mundo contínuo.

			Tenemos la señal contínua, y lo que hacemos es tomar, por ejemplo, el intervalo $[0,2\pi]$ dividido en $N$ trozos:
			\[x_j = \frac{2\pi}{N}j, \quad j=0,1,2,…,N-1\]
			Y el sampling que hemos realizado:
			\[ y_j = f \left(\frac{2\pi}{N}j \right)\]

			\textbf{Idea 1}\\
			Supongamos que existe:

			\[f(x) = \sum_{k=-\infty}^\infty c_k \
			e^{ikx} \quad \text{ con } c_k \text{ desconocido } \]
			Y tenemos el sistema con $N$ ecuaciones
			\begin{align*}
				y_0 &= f(x_0) = f(0) = \sum_{k=-\infty}^\infty c_k \\
				y_1 &= f(x_1) = f\left(\frac{2\pi}{N}\right) = \sum_{k=-\infty}^\infty c_k \left( e^{i\frac{2\pi}{N}} \right)^k \eqreason{$e^{i\frac{2\pi}{N}} \equiv \omega $} \sum_{k=-\infty}^\infty c_k \ \omega^k \\
				\vdots \\
				y_p &= f(x_p) = f\left(\frac{2\pi}{N}p\right) = \sum_{k=-\infty}^\infty c_k \left(\omega^p\right)^k
			\end{align*}

			Tenemos $N$ ecuaciones e infinitas incógnitas $c_k$. Los $c_k$ son valores exactos, pero podemos aspirar a encontrar $N$ valores aproximados $z_k$. Veamos el sistema aproximado:
			\begin{align*}
				y_0 &= \sum_{k=0}^{n-1} z_k\\
				y_1 &= \sum_{k=0}^{n-1} z_k\ \omega^k\\
				y_{N-1} &= \sum_{k=0}^{n-1} z_k \left(\omega^{N-1}\right)^k
			\end{align*}


			\obs $\omega = e^{i \frac{2\pi}{N}}$, \quad $\omega^N = e^{2 \pi i} = 1$, \quad $\omega^{N+k} = \omega^k$ periódico con periodo $N$


			Todo lo que tenemos es $N$-periódico así que cualquier bloque de longitud $N$ nos vale. Suponiendo que los datos $\set{y_i}$ también los extiendo de manera $N$-periódica a izquierda y derecha, tenemos que nos vale cualquier conjunto contiguo de índices de longitud $N$.

			\[ k=0,1,2,…,N-1 \text{ equivalente a } k = -\frac{N}{2}, -\frac{N}{2}+1, … , \frac{N}{2} \]


			Formulación matricial:
			\[ \bar{Y} = \left(
			\begin{matrix}
				1 & 1 & 1 & … & 1 \\
				1 & \omega & \omega^2 & … & \omega^{N-1} \\
				· & \omega^2 & \omega^4 & … & \omega^{(N-1)2} \\
				· & · & · & · & · \\
				1 & \omega^{N-1} & (\omega^2)^{N-1} & … & \omega^{(N-1)(N-1)}
			\end{matrix} \right) \bar{Z}
			\]


			Tenemos el siguiente lema de ortogonalidad:
			\begin{lemma}{Lema de ortogonalidad}
			\[ \sum_{s=0}^{N-1} \omega^{js}\ \omega^{-LS} =
			\begin{cases}
				0 & L \neq j\\
				N & L = j
			\end{cases} \]
			\end{lemma}

			Recapitulando un poco, lo que tenemos hasta ahora es
			\[ f(x) \approx \sum_{s=0}^{N-1} z_k \ e^{ikx} \eqreason[\longrightarrow]{$M << N-1$} \sum_{s=0}^{M} z_k \ e^{ikx} \]
			y esperamos que sea una aproximación lo {\it bastante buena}.

			Veamos la demostración del lema, que es muy cortita:
			\begin{proof}
			\begin{gather*}
				\omega^{js} \omega^{-LS} = \omega^{(j-L)S} = e^{i\frac{2\pi}{N}(j-L)s}\\
				\sum_{s=0}^{N-1} \omega^{js} \omega^{-LS} = \sum_{s=0}^{N-1} e^{i\frac{2\pi}{N}(j-L)s} \eqreason{prog geométrica}
				\begin{cases}
					N & j = L\\
					0 & j \neq L
				\end{cases}\\
			\end{gather*}
			Ya que si $j\neq L$, la suma nos da
			\[ 1 - e^{i \frac{2 \pi}{N} (j-L) N} = 1 - e^{i {2 \pi} (j-L)} \eqreason{múltiplo de $2\pi$} 1 - 1 = 0 \]

			\end{proof}

			% clase 14/3/2016

			% La clase empieza con un comentario que amplia información de una sección anterior, que no he copiado.

			\newpage % FIXME: puede romper la estructura del documento
			\textbf{Aplicación:} cálculo de $\gor{z}$.

				Si fijamos s, tenemos para un $j$ concreto:
				\[ y_j\ \omega^{-js} = \sum_{k=0}^{n-1} z_k\ \omega^{jk}\ \omega^{-js} \]
				luego sumando en $j$:
				\[ \sum_{j=0}^{n-1} y_j\ \omega^{-js} = \sum_{j=0}^{n-1} \sum_{k=0}^{n-1} z_k\ \omega^{jk}\ \omega^{-js} = \sum_{k=0}^{n-1} z_k\ \sum_{j=0}^{n-1} \omega^{jk}\ \omega^{-js}
				\]
				ya que aplicando el lema tenemos:
				\[\sum_{j=0}^{n-1} \omega^{jk}\ \omega^{-js} = \begin{cases}
					0 & s \neq k\\
					N & s = k  \end{cases} \]
				\obs El cambio de sumatorios se puede hacer por el Teorema de Fubini para sumas finitas: da igual sumar primero en $j$ y luego en $k$ que al revés.

				\textbf{Conclusión}
				\begin{align*}
					 n z_s &= \sum_{j=0}^{n-1} y_j \omega^{-js} \\
					 z_s &= \frac{1}{n} \sum_{j=0}^{n-1} y_j \omega^{-js} \longrightarrow O(n^2) \text{ operaciones} \\
					 y_j &= \sum_{k=0}^{n-1} z_k\ \omega^{jk}
				\end{align*}
				\obs Ni $z_s$ ni $y_j$ están en una base ortonormal. A $\set{\gor{z}}$ se le llama la \concept{Transformada\IS discreta (de Fourier) de orden $n$} de $\set{\gor{y}}$

				\[ \gor{z} = F_n (\gor{y}) \]



			\textbf{Idea 2:}

			\[f(x) = \sum_{k=-\infty}^{\infty} c_k e^{ikx} \]
			\[c_k= \frac{1}{2\pi} \int_0^{2\pi} f(x) e^{-ikx} dx ≈ \frac{1}{2\pi} \sum_{j=0}^{n-1} f(x_j) e^{-ikxj} \frac{2\pi}{n}  \]
			Donde hemos aproximado la integral utilizando la regla de cuadratura del rectágulo.
			\[\sum_{j=0}^{n-1} f(x_j) e^{-ikxj} \frac{1}{n} \eqreason{$x=\frac{2\pi j}{n}$} \frac{1}{n} \sum_{j=0}^{n-1}  y_j\ \omega^{-kj} \equiv z_k  \]
			Tomando $k = n$:
			\[c_n \underbrace{\eqexpl[≈]{?}}_{\text{En general no}} \frac{1}{n} \sum_{j=0}^{n-1} y_j\ \omega^{-nj}  \eqreason{$\omega^n \equiv 1$} \frac{1}{n} \sum_{j=0}^{n-1} y_j  = z_0   \]

			En general $z_{n+l} = z_{l}$.

			Esta es una aproximación válida si  $k << n$.

			Un problema que puede ocurrir al truncar la serie de Fourier es el \concept[Aliasing]{aliasing}: frecuencias altas se quedan en bajas, y al interpolar nos queda una curva que no se parece a la señal muestreada. Este fenómeno se ilustra en la \fref{fig:Aliasing}.
			\begin{figure}[hbtp]
				\inputtikz{Aliasing}
				\caption{Ilustración del aliasing}
				\label{fig:Aliasing}
			\end{figure}


			\textbf{Relación precisa entre $\{c_k\}$ y $\{z_k\}$}

			\[ f(x) = \sum_{L=-\infty}^\infty c_L \cdot e^{iLx}  \]

			\begin{gather*}
			z_k = \frac{1}{n}  \sum_{j=0}^{n-1} y_j\ \omega^{-kj} = \frac{1}{n} \sum_{j=0}^{n-1}  f(x_j)\ \omega^{-kj} = \frac{1}{n} \sum_{j=0}^{n-1} f\left( \frac{2\pi}{n} j \right) \omega^{-kj}  \\
			= \frac{1}{n} \sum_{j=0}^{n-1} \sum_{L=-\infty}^{\infty} c_L \cdot \underbrace{e^{iL\frac{2\pi}{n}j}}_{w^{Lj}} \omega^{-kj} = \frac{1}{n} \sum_{L=-\infty}^{\infty} c_L \sum_{j=0}^{n-1} \omega^{Lj} \omega^{-kj}
			\end{gather*}

			No podemos aplicar tal cual el lema de ortogonalidad porque la $L$ no va entre 0 y $n$. Pero podemos descomponer la suma en trozos de longitud $n$. En cada uno de ellos solo uno de los sumandos será distinto de 0.

			\[ \omega^{Lj} = \omega^{(L+n)j} = \omega^{(L+2n)j} … \]
			luego
			\[ \frac{1}{n} \sum_{L=-\infty}^{\infty} c_L \sum_{j=0}^{n-1} \omega^{Lj} \omega^{-kj} = \frac{1}{n} n \{ c_k + c_{k+n} + c_{k+2n} + … \}  \]

			\[ z_k = c_k + c_{k+n} + c_{k+2n} + c_{k+3n} + … = \sum_{k \mod n} c_k  \]

			Por Riemman-Lebesgue: $C_k \convs[][k] 0$

			Esto nos indica que el muestreo puede hacer que una onda de mucha frecuencia (inaudible) en nuestros datos baje de frecuencia (hasta ser audible) al tomar la transformada de Fourier.


			\obs Veamos una función $f(x) = \sum\limits_{k=-\infty}^{\infty} c_k e^{ikx}$.

			\[ \left|  f - \sum_{k=-\infty}^{\infty} c_k e^{ikx} \right| = 0 \]
			\[ = \left|  f - \left( \sum_{|k|\leq n} + \sum_{|k| > n} \right) \right| \leq \left|  f- \sum_{|k| \leq n} \right| + \left| \sum_{|k| > n} \right| \]
			\[ \left| f- \sum_{|k| \leq n} c_k e^{ikx} \right| \leq \left| \sum_{|k| > n} c_k e^{ikx} \right| \leq \sum_{|k| > n} |c_k|  \]

			Esto nos da una cota del error que cometemos en el desarrollo de Fourier, en base a las $c_k$. Tenemos el siguiente teorema, que nos dice que si hacemos ese mismo cálculo con las $z$s, el error no es muy malo.
			\begin{theorem}
				\[ \left| f - \sum_{|k|< n} z_k e^{ikx}  \right| \leq 2 \sum_{|k| > n } |c_k| \]
			\end{theorem}


			\subsubsubsection{Cálculo efectivo (FFT)}
			\obs Algoritmo publicado por Cooley-Tukey en 1965. En 1805 Gauss ya lo usaba, y se publicó en latín a título póstumo en 1865.

			\[ \bar{u} = (u_0, u_1, … u_{n-1}) \rightarrow F_n (\bar{u})  \]
			\[ \bar{v} = (v_0, v_1, … v_{n-1}) \rightarrow F_n (\bar{v})  \]

			Entonces:
			\[ \bar{y} = (y_0,y_1,…,y_{2n-1}) \equiv (u_0,v_0,u_1,v_1, … u_{n-1},v_{n-1})  \]

			Pero ¿qué pasa con $f_{2n}(\bar{y})$?

			Tenemos
			\[ \left(F_{2n} (\bar{y})\right)_k = \left(F_{n} (\bar{u})\right)_k + \left(F_{n} (\bar{v})\right)_k \omega_{2n}^{-k}, k=0,1,…,n-1 \]

			\[ \left(F_{2n} (\bar{y})\right)_{k+n} = \left(F_{n} (\bar{u})\right)_k - \left(F_{n} (\bar{v})\right)_k \omega_{2n}^{-k}, k=0,1,…,n-1 \]
			\obs $\omega_{2n} = e^{i\frac{2\pi}{2n}}$, $\omega_{n} = e^{i \frac{2\pi}{n}}$

			\begin{proof}
				\[ w_{2n}^2 = \omega_{n} \]
				Separamos la suma en indices pares e impares y listo.
			\end{proof}

			\begin{figure}[hbtp]
				\inputtikz{FFT}
				\caption{Ejemplo del algoritmo FFT}
			\end{figure}

			Este método es $O(n \log n)$, pero solo funciona cuando el número de coeficientes es una potencia de 2.


